{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3999e791",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input, Add, Lambda, ReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import cv2\n",
    "\n",
    "# Define paths (adjust if needed)\n",
    "base_dir = 'data'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# Classes\n",
    "classes = ['angry', 'happy', 'neutral']\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Image size\n",
    "img_size = (48, 48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b249c315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12940 images belonging to 3 classes.\n",
      "Found 3235 images belonging to 3 classes.\n",
      "Found 3965 images belonging to 3 classes.\n",
      "Class weights: {0: np.float64(1.3496036712557364), 1: np.float64(0.7472857472857473), 2: np.float64(1.085934877475663)}\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation for train\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Split 20% of train for validation\n",
    ")\n",
    "\n",
    "# Validation and test: only rescale\n",
    "val_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Train generator\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=64,  # Balanced for M3\n",
    "    color_mode='grayscale',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation generator (from train split)\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=64,\n",
    "    color_mode='grayscale',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=64,\n",
    "    color_mode='grayscale',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False  # For evaluation\n",
    ")\n",
    "\n",
    "# Compute class weights for imbalance\n",
    "class_indices = train_generator.class_indices  # {'angry':0, 'happy':1, 'neutral':2}\n",
    "num_classes = len(classes)\n",
    "y_train = train_generator.classes\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weights_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05f0f4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m 20/203\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 88ms/step - accuracy: 0.3180 - loss: 2.0210"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m checkpoint = ModelCheckpoint(\u001b[33m'\u001b[39m\u001b[33mmodel1_best.h5\u001b[39m\u001b[33m'\u001b[39m, monitor=\u001b[33m'\u001b[39m\u001b[33mval_accuracy\u001b[39m\u001b[33m'\u001b[39m, save_best_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m history1 = \u001b[43mmodel1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on time; early stop will halt\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weights_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Plot training\u001b[39;00m\n\u001b[32m     41\u001b[39m plt.plot(history1.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Build custom CNN\n",
    "model1 = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(48,48,1)),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2,2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model1.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('model1_best.h5', monitor='val_accuracy', save_best_only=True)\n",
    "\n",
    "# Train\n",
    "history1 = model1.fit(\n",
    "    train_generator,\n",
    "    epochs=50,  # Adjust based on time; early stop will halt\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop, checkpoint]\n",
    ")\n",
    "\n",
    "# Plot training\n",
    "plt.plot(history1.history['accuracy'], label='train_acc')\n",
    "plt.plot(history1.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c9bfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For transfer, convert grayscale to RGB by repeating channels\n",
    "def grayscale_to_rgb(x):\n",
    "    return np.repeat(x, 3, axis=-1)  # Apply in generator if needed, but Keras can handle with input_shape\n",
    "\n",
    "# Build transfer model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(48,48,3))\n",
    "base_model.trainable = False  # Freeze base\n",
    "\n",
    "inputs = Input(shape=(48,48,1))\n",
    "x = tf.keras.layers.Concatenate()([inputs, inputs, inputs])  # Grayscale to RGB\n",
    "x = base_model(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model2 = Model(inputs, outputs)\n",
    "model2.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Unfreeze some layers for fine-tuning (after initial training if needed)\n",
    "# For now, train top layers\n",
    "\n",
    "# Train\n",
    "history2 = model2.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop, ModelCheckpoint('model2_best.h5', monitor='val_accuracy', save_best_only=True)]\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.plot(history2.history['accuracy'], label='train_acc')\n",
    "plt.plot(history2.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Fine-tune: Unfreeze top layers\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:  # Fine-tune last 20\n",
    "    layer.trainable = False\n",
    "\n",
    "model2.compile(optimizer=Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])  # Lower LR\n",
    "history2_fine = model2.fit(\n",
    "    train_generator,\n",
    "    epochs=20,  # Shorter for fine-tune\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop, ModelCheckpoint('model2_fine_best.h5', monitor='val_accuracy', save_best_only=True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a6d5248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not locate function 'grayscale_to_rgb'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'builtins', 'class_name': 'function', 'config': 'grayscale_to_rgb', 'registered_name': 'function'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_model\n\u001b[32m      3\u001b[39m best_model1 = load_model(\u001b[33m'\u001b[39m\u001b[33mmodel1_best.h5\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m best_model2 = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel2_fine_best.h5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Or model2_best.h5 if fine-tune not better\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Evaluate on test set\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_model\u001b[39m(model, generator):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/saving/saving_api.py:196\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib.load_model(\n\u001b[32m    190\u001b[39m         filepath,\n\u001b[32m    191\u001b[39m         custom_objects=custom_objects,\n\u001b[32m    192\u001b[39m         \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28mcompile\u001b[39m,\n\u001b[32m    193\u001b[39m         safe_mode=safe_mode,\n\u001b[32m    194\u001b[39m     )\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith((\u001b[33m\"\u001b[39m\u001b[33m.h5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.hdf5\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_h5_format\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model_from_hdf5\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith(\u001b[33m\"\u001b[39m\u001b[33m.keras\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    204\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    205\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mzip file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/legacy/saving/legacy_h5_format.py:137\u001b[39m, in \u001b[36mload_model_from_hdf5\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    135\u001b[39m safe_mode_scope = serialization_lib.SafeModeScope(safe_mode)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m legacy_scope, safe_mode_scope:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     model = \u001b[43msaving_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# set weights\u001b[39;00m\n\u001b[32m    142\u001b[39m     load_weights_from_hdf5_group(f[\u001b[33m\"\u001b[39m\u001b[33mmodel_weights\u001b[39m\u001b[33m\"\u001b[39m], model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/legacy/saving/saving_utils.py:83\u001b[39m, in \u001b[36mmodel_from_config\u001b[39m\u001b[34m(config, custom_objects)\u001b[39m\n\u001b[32m     80\u001b[39m         function_dict[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mclosure\u001b[39m\u001b[33m\"\u001b[39m] = function_config[\u001b[32m2\u001b[39m]\n\u001b[32m     81\u001b[39m         config[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m] = function_dict\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlayer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/legacy/saving/serialization.py:488\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[39m\n\u001b[32m    485\u001b[39m custom_objects = custom_objects \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcustom_objects\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec.args:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     deserialized_obj = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mobject_registration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration.CustomObjectScope(custom_objects):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/models/model.py:660\u001b[39m, in \u001b[36mModel.from_config\u001b[39m\u001b[34m(cls, config, custom_objects)\u001b[39m\n\u001b[32m    655\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[32m    656\u001b[39m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[32m    657\u001b[39m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[32m--> \u001b[39m\u001b[32m660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    661\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[32m    665\u001b[39m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[32m    666\u001b[39m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    669\u001b[39m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[32m    670\u001b[39m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[32m    671\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/models/functional.py:558\u001b[39m, in \u001b[36mfunctional_from_config\u001b[39m\u001b[34m(cls, config, custom_objects)\u001b[39m\n\u001b[32m    556\u001b[39m \u001b[38;5;66;03m# First, we create all layers and enqueue nodes to be processed\u001b[39;00m\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_data \u001b[38;5;129;01min\u001b[39;00m functional_config[\u001b[33m\"\u001b[39m\u001b[33mlayers\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[43mprocess_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;66;03m# Then we process nodes in order of layer depth.\u001b[39;00m\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# Nodes that cannot yet be processed (if the inbound node\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# does not yet exist) are re-enqueued, and the process\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# is repeated until all nodes are processed.\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m unprocessed_nodes:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/models/functional.py:521\u001b[39m, in \u001b[36mfunctional_from_config.<locals>.process_layer\u001b[39m\u001b[34m(layer_data)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;66;03m# Instantiate layer.\u001b[39;00m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmodule\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m layer_data:\n\u001b[32m    519\u001b[39m     \u001b[38;5;66;03m# Legacy format deserialization (no \"module\" key)\u001b[39;00m\n\u001b[32m    520\u001b[39m     \u001b[38;5;66;03m# used for H5 and SavedModel formats\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m521\u001b[39m     layer = \u001b[43msaving_utils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    522\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    525\u001b[39m     layer = serialization_lib.deserialize_keras_object(\n\u001b[32m    526\u001b[39m         layer_data, custom_objects=custom_objects\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/legacy/saving/saving_utils.py:83\u001b[39m, in \u001b[36mmodel_from_config\u001b[39m\u001b[34m(config, custom_objects)\u001b[39m\n\u001b[32m     80\u001b[39m         function_dict[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mclosure\u001b[39m\u001b[33m\"\u001b[39m] = function_config[\u001b[32m2\u001b[39m]\n\u001b[32m     81\u001b[39m         config[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m] = function_dict\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mserialization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodule_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODULE_OBJECTS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mALL_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprintable_module_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlayer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/legacy/saving/serialization.py:488\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[39m\n\u001b[32m    485\u001b[39m custom_objects = custom_objects \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    487\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcustom_objects\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m arg_spec.args:\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     deserialized_obj = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcls_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mobject_registration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGLOBAL_CUSTOM_OBJECTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m object_registration.CustomObjectScope(custom_objects):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/layers/core/lambda_layer.py:200\u001b[39m, in \u001b[36mLambda.from_config\u001b[39m\u001b[34m(cls, config, custom_objects, safe_mode)\u001b[39m\n\u001b[32m    198\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m] = fn\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     config[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mserialization_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33moutput_shape\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[32m    204\u001b[39m     fn_config = config[\u001b[33m\"\u001b[39m\u001b[33moutput_shape\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:693\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(config, custom_objects, safe_mode, **kwargs)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m class_name == \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    692\u001b[39m     fn_name = inner_config\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Below, handling of all classes.\u001b[39;00m\n\u001b[32m    703\u001b[39m \u001b[38;5;66;03m# First, is it a shared object?\u001b[39;00m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mshared_object_id\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:836\u001b[39m, in \u001b[36m_retrieve_class_or_fn\u001b[39m\u001b[34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[39m\n\u001b[32m    829\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[32m    830\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    831\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not deserialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m because \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mits parent module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be imported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    833\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    834\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    837\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    838\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMake sure custom classes are decorated with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    839\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    840\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    841\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Could not locate function 'grayscale_to_rgb'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'builtins', 'class_name': 'function', 'config': 'grayscale_to_rgb', 'registered_name': 'function'}"
     ]
    }
   ],
   "source": [
    "# Load best models\n",
    "from tensorflow.keras.models import load_model\n",
    "best_model1 = load_model('model1_best.h5')\n",
    "best_model2 = load_model('model2_fine_best.h5')  # Or model2_best.h5 if fine-tune not better\n",
    "\n",
    "# Evaluate on test set\n",
    "def evaluate_model(model, generator):\n",
    "    loss, acc = model.evaluate(generator)\n",
    "    print(f\"Test Accuracy: {acc}\")\n",
    "    y_pred = np.argmax(model.predict(generator), axis=1)\n",
    "    y_true = generator.classes\n",
    "    print(classification_report(y_true, y_pred, target_names=classes))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.imshow(cm, cmap='Blues')\n",
    "    plt.xticks(range(num_classes), classes)\n",
    "    plt.yticks(range(num_classes), classes)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Model 1 Evaluation:\")\n",
    "evaluate_model(best_model1, test_generator)\n",
    "\n",
    "print(\"Model 2 Evaluation:\")\n",
    "evaluate_model(best_model2, test_generator)\n",
    "\n",
    "# Choose best: Compare F1-scores (macro avg for balance). Model 2 likely better due to pre-trained features.\n",
    "# Justify: Custom CNN is simple/from-scratch (good for understanding), Transfer is efficient/advanced (higher scores, real-world deployable on mobile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "223dddd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12940 images belonging to 3 classes.\n",
      "Found 3235 images belonging to 3 classes.\n",
      "Found 3965 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Face alignment and noise reduction function (Fixed to return (48, 48, 1) shape)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "def align_and_denoise(image):\n",
    "    if len(image.shape) == 3 and image.shape[2] == 1:\n",
    "        image = image.squeeze()  # Remove channel if single-channel\n",
    "    gray = image.astype(np.uint8)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "    if len(faces) > 0:\n",
    "        x, y, w, h = faces[0]\n",
    "        face = gray[y:y+h, x:x+w]\n",
    "        face = cv2.GaussianBlur(face, (3, 3), 0)  # Light denoising\n",
    "        face = cv2.resize(face, img_size).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "        return np.expand_dims(face, axis=-1)  # Add channel dimension to (48, 48, 1)\n",
    "    face = cv2.resize(gray, img_size).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return np.expand_dims(face, axis=-1)  # Add channel dimension to (48, 48, 1)\n",
    "\n",
    "# Custom ImageDataGenerator with proper constructor\n",
    "class CustomImageDataGenerator(ImageDataGenerator):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # Properly call parent constructor\n",
    "\n",
    "    def standardize(self, x):\n",
    "        x = align_and_denoise(x)\n",
    "        return x  # Return pre-normalized and shaped array\n",
    "\n",
    "# Train generator with advanced augmentation\n",
    "train_datagen = CustomImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=32,  # Optimized for 8GB RAM\n",
    "    color_mode='grayscale',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Test generator\n",
    "test_datagen = CustomImageDataGenerator()\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    classes=classes,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9353b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: np.float64(1.3496036712557364), 1: np.float64(0.7472857472857473), 2: np.float64(1.085934877475663)}\n"
     ]
    }
   ],
   "source": [
    "# Compute class weights\n",
    "y_train = train_generator.classes\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "print(f\"Class weights: {class_weights_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad9d3ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclic Learning Rate Callback\n",
    "class CyclicLR(Callback):\n",
    "    def __init__(self, base_lr=0.0001, max_lr=0.001, step_size=2000.):\n",
    "        super(CyclicLR, self).__init__()\n",
    "        self.base_lr = base_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.step_size = step_size\n",
    "        self.clr_iterations = 0.\n",
    "        self.trn_iterations = 0.\n",
    "        self.history = {}\n",
    "\n",
    "    def clr(self):\n",
    "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
    "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
    "        return self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, 1 - x)\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.trn_iterations += 1\n",
    "        self.clr_iterations += 1\n",
    "        clr = self.clr()\n",
    "        self.model.optimizer.learning_rate = clr\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34773cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m 36/405\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 77ms/step - accuracy: 0.3547 - loss: 1.6861"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m clr = CyclicLR(base_lr=\u001b[32m0.0001\u001b[39m, max_lr=\u001b[32m0.001\u001b[39m, step_size=\u001b[32m2000.\u001b[39m)\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m history1 = \u001b[43mmodel1\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weights_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n\u001b[32m     53\u001b[39m plt.plot(history1.history[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m], label=\u001b[33m'\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:377\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    375\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    376\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    378\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:220\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    218\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    219\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    222\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Campus/ComputerVision/emotion-recognition-project/env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Residual block function\n",
    "def residual_block(x, filters):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(filters, (3, 3), padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    shortcut = Conv2D(filters, (1, 1), padding='same')(shortcut) if shortcut.shape[-1] != filters else shortcut\n",
    "    x = Add()([shortcut, x])\n",
    "    x = ReLU()(x)  # Using tf.keras.layers.ReLU\n",
    "    return x\n",
    "\n",
    "# Build model\n",
    "inputs = Input(shape=(48, 48, 1))\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D(2, 2)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = residual_block(x, 64)\n",
    "x = MaxPooling2D(2, 2)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = residual_block(x, 128)\n",
    "x = MaxPooling2D(2, 2)(x)\n",
    "x = Dropout(0.25)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model1 = Model(inputs, outputs)\n",
    "optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)  # Gradient clipping\n",
    "model1.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint1 = ModelCheckpoint('model1_best.h5', monitor='val_accuracy', save_best_only=True)\n",
    "clr = CyclicLR(base_lr=0.0001, max_lr=0.001, step_size=2000.)\n",
    "\n",
    "# Train\n",
    "history1 = model1.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop, checkpoint1, clr]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history1.history['accuracy'], label='train_acc')\n",
    "plt.plot(history1.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.title('Model 1 Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746f8efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grayscale to RGB conversion\n",
    "def grayscale_to_rgb(x):\n",
    "    return tf.repeat(x, 3, axis=-1)\n",
    "\n",
    "# Build model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = Input(shape=(48, 48, 1))\n",
    "x = Lambda(grayscale_to_rgb)(inputs)\n",
    "x = base_model(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "model2 = Model(inputs, outputs)\n",
    "optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train initial\n",
    "checkpoint2 = ModelCheckpoint('model2_best.h5', monitor='val_accuracy', save_best_only=True)\n",
    "history2 = model2.fit(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop, checkpoint2, clr]\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "optimizer = Adam(learning_rate=0.0001, clipnorm=1.0)\n",
    "model2.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpoint2_fine = ModelCheckpoint('model2_fine_best.h5', monitor='val_accuracy', save_best_only=True)\n",
    "history2_fine = model2.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    class_weight=class_weights_dict,\n",
    "    callbacks=[early_stop, checkpoint2_fine, clr]\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history2_fine.history['accuracy'], label='train_acc')\n",
    "plt.plot(history2_fine.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.title('Model 2 Fine-Tuning History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "482c6daf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Use the trained model directly\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m best_model2 = \u001b[43mmodel2\u001b[49m  \u001b[38;5;66;03m# After history2_fine\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating Model 1 (Custom CNN)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m acc1 = evaluate_model(best_model1, test_generator, \u001b[33m\"\u001b[39m\u001b[33mModel 1 (Custom CNN)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'model2' is not defined"
     ]
    }
   ],
   "source": [
    "# Use the trained model directly\n",
    "best_model2 = model2  # After history2_fine\n",
    "\n",
    "print(\"Evaluating Model 1 (Custom CNN)\")\n",
    "acc1 = evaluate_model(best_model1, test_generator, \"Model 1 (Custom CNN)\")\n",
    "\n",
    "print(\"Evaluating Model 2 (MobileNetV2)\")\n",
    "acc2 = evaluate_model(best_model2, test_generator, \"Model 2 (MobileNetV2)\")\n",
    "\n",
    "# Choose best model\n",
    "best_model = best_model2 if acc2 > acc1 else best_model1\n",
    "print(f\"Best Model: {'Model 2 (MobileNetV2)' if acc2 > acc1 else 'Model 1 (Custom CNN)'} with Accuracy: {max(acc1, acc2):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0284f149",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'acc2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load best model based on evaluation\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43macc2\u001b[49m > acc1:\n\u001b[32m      3\u001b[39m     best_model = load_model(\u001b[33m'\u001b[39m\u001b[33mmodel2_fine_best.h5\u001b[39m\u001b[33m'\u001b[39m, custom_objects={\u001b[33m'\u001b[39m\u001b[33mgrayscale_to_rgb\u001b[39m\u001b[33m'\u001b[39m: grayscale_to_rgb})\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'acc2' is not defined"
     ]
    }
   ],
   "source": [
    "# Load best model based on evaluation\n",
    "if acc2 > acc1:\n",
    "    best_model = load_model('model2_fine_best.h5', custom_objects={'grayscale_to_rgb': grayscale_to_rgb})\n",
    "else:\n",
    "    best_model = load_model('model1_best.h5')\n",
    "\n",
    "# Real-time inference\n",
    "cap = cv2.VideoCapture(0)  # Webcam\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_resized = cv2.resize(roi_gray, (48, 48)).astype(np.float32) / 255.0\n",
    "        roi_resized = roi_resized.reshape(1, 48, 48, 1)\n",
    "        pred = best_model.predict(roi_resized, verbose=0)\n",
    "        label = classes[np.argmax(pred)]\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19527587",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_22\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_22\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_31[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_14    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_19          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_14… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_32 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_32[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_33 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │ batch_normalizat… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_34 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ dropout_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │ conv2d_33[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_34[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_15    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_20          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_15… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │ <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_35 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_35[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_36 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │ batch_normalizat… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_37 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │ dropout_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │ conv2d_36[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_37[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ add_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_16    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ re_lu_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_21          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d_16… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_21[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,179,904</span> │ flatten_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_22          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">771</span> │ dropout_22[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_11      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m1\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_31 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m,    │        \u001b[38;5;34m320\u001b[0m │ input_layer_11[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ conv2d_31[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_14    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_19          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_14… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_32 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │     \u001b[38;5;34m18,496\u001b[0m │ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_32[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_33 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │     \u001b[38;5;34m36,928\u001b[0m │ batch_normalizat… │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_34 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │      \u001b[38;5;34m2,112\u001b[0m │ dropout_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │        \u001b[38;5;34m256\u001b[0m │ conv2d_33[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_7 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_34[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu_6 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│                     │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_15    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ re_lu_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_20          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_15… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │ \u001b[38;5;34m64\u001b[0m)               │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_35 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │     \u001b[38;5;34m73,856\u001b[0m │ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv2d_35[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_36 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │    \u001b[38;5;34m147,584\u001b[0m │ batch_normalizat… │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ conv2d_37 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │      \u001b[38;5;34m8,320\u001b[0m │ dropout_20[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │        \u001b[38;5;34m512\u001b[0m │ conv2d_36[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_8 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_37[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │ batch_normalizat… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ re_lu_7 (\u001b[38;5;33mReLU\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ add_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│                     │ \u001b[38;5;34m128\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ max_pooling2d_16    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ re_lu_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_21          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m) │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d_16… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ flatten_5 (\u001b[38;5;33mFlatten\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ dropout_21[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │  \u001b[38;5;34m1,179,904\u001b[0m │ flatten_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │      \u001b[38;5;34m1,024\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_22          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │        \u001b[38;5;34m771\u001b[0m │ dropout_22[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,470,981</span> (5.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,470,981\u001b[0m (5.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,469,635</span> (5.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,469,635\u001b[0m (5.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,344</span> (5.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,344\u001b[0m (5.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('/Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/model1_best.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "692a1209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/saved_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/saved_model'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 48, 48, 1), dtype=tf.float32, name='input_layer_11')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  6302150160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302150544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302151120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302152080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302151888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302151312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302151696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302150928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302149584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302153232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302153040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302150736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302152848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302152272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302154000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302153424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302152464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302150352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302154576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302153616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302153808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302151504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302152656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303352016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302154192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6302154384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303351632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303351248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303352784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303352208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303351440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303353552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303353360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303352400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303353168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303352976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303351824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303354704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303354512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303351056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303354320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  6303353744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('/Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/model1_best.h5')\n",
    "model.export('/Users/bhanukapannipitiya/Desktop/Campus/ComputerVision/emotion-recognition-project/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e185fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model1_best.h5...\n",
      "Model loaded successfully!\n",
      "Testing model with sample image...\n",
      "Prediction correct: None\n"
     ]
    }
   ],
   "source": [
    "# Demo with Model1_best only\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model\n",
    "print(\"Loading model1_best.h5...\")\n",
    "model = load_model('model1_best.h5')\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Define classes and image size\n",
    "classes = ['angry', 'happy', 'neutral']\n",
    "img_size = (48, 48)\n",
    "\n",
    "# Function to preprocess image for prediction\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess image for model prediction\"\"\"\n",
    "    # Convert to grayscale if needed\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Resize to model input size\n",
    "    image = cv2.resize(image, img_size)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Add channel dimension and batch dimension\n",
    "    image = np.expand_dims(image, axis=-1)  # (48, 48, 1)\n",
    "    image = np.expand_dims(image, axis=0)   # (1, 48, 48, 1)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Function to predict emotion from image\n",
    "def predict_emotion(image):\n",
    "    \"\"\"Predict emotion from preprocessed image\"\"\"\n",
    "    predictions = model.predict(image, verbose=0)\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "    confidence = predictions[0][predicted_class]\n",
    "    emotion = classes[predicted_class]\n",
    "    \n",
    "    return emotion, confidence, predictions[0]\n",
    "\n",
    "# Test with a sample image from the dataset\n",
    "def test_with_sample_image():\n",
    "    \"\"\"Test the model with a sample image from the test dataset\"\"\"\n",
    "    import os\n",
    "    import random\n",
    "    \n",
    "    # Get a random test image\n",
    "    test_dir = 'data/test'\n",
    "    emotion_folders = os.listdir(test_dir)\n",
    "    random_emotion = random.choice(emotion_folders)\n",
    "    \n",
    "    if random_emotion in classes:\n",
    "        emotion_path = os.path.join(test_dir, random_emotion)\n",
    "        image_files = [f for f in os.listdir(emotion_path) if f.endswith('.jpg')]\n",
    "        random_image = random.choice(image_files)\n",
    "        image_path = os.path.join(emotion_path, random_image)\n",
    "        \n",
    "        # Load and display the image\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Preprocess for prediction\n",
    "        processed_image = preprocess_image(image)\n",
    "        \n",
    "        # Predict emotion\n",
    "        predicted_emotion, confidence, all_predictions = predict_emotion(processed_image)\n",
    "        \n",
    "        # Display results\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.title(f'Original Image\\nTrue: {random_emotion}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Grayscale processed image\n",
    "        plt.subplot(1, 3, 2)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        plt.imshow(gray_image, cmap='gray')\n",
    "        plt.title('Processed (48x48)')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Prediction results\n",
    "        plt.subplot(1, 3, 3)\n",
    "        emotions = classes\n",
    "        confidences = all_predictions\n",
    "        colors = ['red' if e == predicted_emotion else 'lightblue' for e in emotions]\n",
    "        \n",
    "        bars = plt.bar(emotions, confidences, color=colors)\n",
    "        plt.title(f'Prediction Results\\nPredicted: {predicted_emotion}\\nConfidence: {confidence:.3f}')\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add confidence values on bars\n",
    "        for bar, conf in zip(bars, confidences):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{conf:.3f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"True emotion: {random_emotion}\")\n",
    "        print(f\"Predicted emotion: {predicted_emotion}\")\n",
    "        print(f\"Confidence: {confidence:.3f}\")\n",
    "        print(f\"All predictions: {dict(zip(emotions, confidences))}\")\n",
    "        \n",
    "        return predicted_emotion == random_emotion\n",
    "\n",
    "# Run the test\n",
    "print(\"Testing model with sample image...\")\n",
    "correct = test_with_sample_image()\n",
    "print(f\"Prediction correct: {correct}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c099f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 10:19:08.331 Python[46895:7196387] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam demo started!\n",
      "Press 'q' to quit, 's' to save current frame\n",
      "Webcam demo ended!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Real-time webcam demo with Model1_best\n",
    "def webcam_demo():\n",
    "    \"\"\"Real-time emotion detection using webcam\"\"\"\n",
    "    import cv2\n",
    "    \n",
    "    # Load face cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam\")\n",
    "        return\n",
    "    \n",
    "    print(\"Webcam demo started!\")\n",
    "    print(\"Press 'q' to quit, 's' to save current frame\")\n",
    "    \n",
    "    frame_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        # Convert to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Detect faces\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            # Draw rectangle around face\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            \n",
    "            # Extract face region\n",
    "            face_roi = gray[y:y+h, x:x+w]\n",
    "            \n",
    "            # Preprocess face for emotion prediction\n",
    "            processed_face = preprocess_image(face_roi)\n",
    "            \n",
    "            # Predict emotion\n",
    "            emotion, confidence, _ = predict_emotion(processed_face)\n",
    "            \n",
    "            # Display prediction on frame\n",
    "            label = f\"{emotion} ({confidence:.2f})\"\n",
    "            cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "        \n",
    "        # Add instructions\n",
    "        cv2.putText(frame, \"Press 'q' to quit, 's' to save\", (10, 30), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display frame\n",
    "        cv2.imshow('Emotion Detection - Model1_best', frame)\n",
    "        \n",
    "        # Handle key presses\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "        elif key == ord('s'):\n",
    "            # Save current frame\n",
    "            filename = f'emotion_detection_frame_{frame_count}.jpg'\n",
    "            cv2.imwrite(filename, frame)\n",
    "            print(f\"Frame saved as {filename}\")\n",
    "            frame_count += 1\n",
    "    \n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Webcam demo ended!\")\n",
    "\n",
    "# Uncomment the line below to run webcam demo\n",
    "webcam_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fded7c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batch test on 10 random samples...\n",
      "Testing model on 10 random samples...\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABc0AAAJSCAYAAAARRO2UAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAr4RJREFUeJzs3Ql8FtW9+P8vOyH7AmEn7Mi+CaKi0qJYvVrvVUvVq9Za1La2ttzeim0Fta3Y1nr5VVEURbtRqV61/hXxqpW6YVGECiogsi8JkD0h7M//dSZPAs93vslMAoQn5PN+vVBmcmbmzJnJYc555vl+m0UikYgAAAAAAAAAAABpfrIrAAAAAAAAAABAvGDSHAAAAAAAAACAKCbNAQAAAAAAAACIYtIcAAAAAAAAAIAoJs0BAAAAAAAAAIhi0hwAAAAAAAAAgCgmzQEAAAAAAAAAiGLSHAAAAAAAAACAKCbNAQAAAAAAAACIaln1FwAAAAAA0DgNnzPc+//+Q/tlTf4aGdJhiLfcP6u/LLhigTRGdy2+S6adPU3atmxb5203Fm302qRoWtEJqRsA4NTWLBKJRE52JQAAAAAAwLGrbbL44OGD0rJ543l3rtndzaTw9kJJa5tW53Nh0hwAcCwIzwIAAAAAwCkqZ1aO3P7a7TJm7hi5/oXrZfHGxdVvpTurdq7yylR5dd2rcva8s2XUY6O8bd7c8GbgMdwEddp9aTLjzRnedn1+10cWfr6w+ucfbPtAvvT7L8nox0bLiEdHyDOfPBOzXZWy/WXeRLlzy0u3eP8f/+R4r747y3fKN174hnzzb9+Uc548RwY/PNj7+TXPXePtd+gjQ+Xi+RdLblnucWk3AEDT1ng+YgYAAAAAAHWWX5Ev//zWP6VZs2bepHlN1heul7v+cZe8+p+vSkqbFFlXsM6btN5420Zp07KNN3m98JqF0jm5s2/b4n3FMjR7qNw94W5ZtG6R3LboNrmo70VStLdIbnrpJll49ULplNxJdu/ZLSMfHSlndjuz1jrP+bc58uiyR+XtG96OedN82Y5l8s4N70hym2RvedakWdI+sb339/veuc8L6eK2BQDgWDBpDgAAAADAKewbw7/hTZgHcZPdbqLcvcldpXmz5rK5eLP0zewrK25ZUeO2Lu74f5z2H97fx3UdJ18UfOH9/b0t73mT8V/581diyru4673Se9X5XK4ceGX1hLkzf+V8+ePHf5S9B/d6f7LaZdV5nwAAaEyaAwAAAABwCktqnVT9dxcH/FDkUPWym2iu4lKend/rfJl/+fw6H6NNizbVE/MtmreoPobb56D2g+S9G9/zbbO1ZGuNdQlzLu9sfkd+t/R3suTGJdIhsYO8uOZFmf7m9DrXHQAAjZjmAAAAAAA0Ee7t7k1Fm2RX+S5v+Y//+mP1zyb1mSSvr39dPs77uHrd0m1Lj+l4LgzLhqIN3n6rrMhdIfsP7ZeOSR29SfVPd33qrf/Dv/4Qs21y62Qp3ltc474LKwq9MpkJmd7+XDgXAACOBybNAQAAAABoIlw88h+f9WMZ8/gYOePxMyQjIaP6Z30y+nhvmd/80s0ybM4wOW32aTLr/VnVP3cxzbeXbq/T8dIT0uXlq1+We9++19vnwNkDZdrr0+Rw5LD31vuDX3lQ/m3+v8npc0+XA4cOxGz7X+P+S87/4/nViUC1C/tcKP2z+kv/h/pXJgzNPpLgFACAY9Es4j7WBQAAAAAAAAAAvGkOAAAAAAAAAEAVJs0BAAAAAAAAAIhi0hwAAAAAAAAAgCgmzQEAAAAAaORyZuV4CTFd0kyXbHP20tnHvM9VO1d5+z1ZXBLS3LLcem/f7O5mUrS36LjWCQDQNLQ82RUAAAAAAADHbsEVC2R4x+GyqWiTDJ0zVMb3GC9Ds4dW//xw5LD3/+bNGsf7c27S/Lyc86RjUkffzxrbuQAAGhcmzQEAAAAAOIX0SOsh/TP7y9r8tfLcZ8/Jyp0rpWx/mWwp3iKvXfua9wb5z9/6uVQcrJAWzVrIryb+Sib0nOBte9fiu+TPK/8sKW1S5Ct9vlKnt7p/+aVfygurX5Bde3bJ9HOmyw0jbvB+9nn+5/KDV38gO8t3yr6D++SmUTfJrWNurd6u8PZCSWub5i1n/TpLPrzpQ/nDv/4g20u3y+RnJ0tCywR56rKnvH3rc/mf9/9H/rHpH3Lg0AGvznMvmSv9s/qfkHYFADQdTJoDAAAAAHAKWZm3UlbvXi3Dsod5E+RLtiyR5Tcvl+ykbFlfuF7u+sdd8up/vupNMq8rWCfjnxwvG2/bKK+vf12e+fQZWXbTMklunSzXPn9tzH4v+vNFcs+Ee2R059Hmcdu0aCNLpyz1jn363NPl2mHXSjNpJlf971Xyp//4kwzIGiB7DuyRMx4/Q8Z2GSundzm9xnOYfu50mbd8XvXb846bND/6XJzbz7pd7r/gfu/vT696Wm5bdJss+s9Fx7E1AQBNEZPmAAAAAACcAqreym7Xqp3M++o86ZvZ11t/Ud+LqieZF61b5E2Un/PkOdXbuRAnm4s3yxsb3pCvDfyaN5nu3DzqZnln8zvV5RZes7DW418z9Brv/25yvGXzll488pJ9JfLJrk/k689+vbpc6f5S+XTXp7VOmtfk6HNxXlv/mjy49EEp3VfqhWwpqCio8z4BANCYNAcAAAAA4BRw9FvZR0tqnVT990gkIuf3Ol/mXz4/cH/NmjWr0/Hbtmxb/XcX9uXg4YPe8TISMmTFLSvMbVy5Q4cPVS/vPbi31mMcfS5uov/WhbfKB1M+kN4ZveXjvI9jPgwAAKC+yJgBAAAAAEATManPJC8Mi5tgrrJ021Lv/xN7TfTCs7i3tt1k92PLHjvm47n44u7N9SeXP1m9zr3pXvVGeJ+MPvLPbf/0/u7ir5cfKK8u57Yr3ltc477dz1q1aCWdkjt59X1o6UPHXF8AABwmzQEAAAAAaCLcJLV7y/zml26WYXOGyWmzT5NZ78+qDn1yxcArZORjI2X03NHSPbW7L6b5h9s/rNPxXJiWl656SZ5b/ZwMfWSoDHp4kNz44o1ScaDC+/n/TPofLw75yEdHyvIdyyUzIbN62++P/b5M+f+myPA5w2VFrv9N9SHZQ+Trg77u7dPFUNf1BQCgvppF3MexAAAAAAAAAACAN80BAAAAAAAAAKjCpDkAAAAAAAAAAFEtq/4CAAAAAABOTQcPH5RfvvVL+cuqv3hxxt2fMV3GyK/P/7WktU2r935v/NuNsmTrEi/h5/OTn6+x3OHIYbntldtk4bqF0kyayQ/O+IHcOuZWs+yidYvkZ3//mew/tF/atWonj/7bozKs4zDvZx9s+0B+8OoPpGx/mbefByY9IF/q+aV61x8AAAuT5gAAAAAAnOJc8s2CigJZcuMSSU9IF5fe7NlPn/XW1XfSPK8sT57+5GkpmVYiLZq3qLXsnz7+k3y6+1NZe+taKd5XLCMeHSETcibIoA6DYsoVVhTKNc9dI2994y3vZ29vettbXvWdVV6d/33Bv8tTlz0lE3tNlLX5a2XiHybKmlvXSEKrhHqdAwAAFsKzAAAAAABwCltXsE6e+eQZefKrT3oT5k6zZs3kykFXSq/0Xt7yb979jQx6eJAMeWSIN0ldvLfYW3/X4rtk8rOT5ZK/XCIDZw+UL/3+S95Ee9HeIpnw+wmy9+BeGfXYKLnvnftqrcOCTxbIlJFTvMn1jIQMmTxosvfWu/ZF4ReSmZBZPZk+vsd42Vy8WT7a8ZHkV+TLrj27vAlzp19mP2/C/5V1rxz3NgMANG1MmgMAAAAAcApzE859M/tKVrss8+evfP6KzFsxT9795ruy8tsrJbFVokx7fVr1z/+59Z/y1Fefkk+/+6l0SOwgj374qDdZvfCahZLcOllW3LJCpp09TeZ8OEemvzndPIab+O6R2qN6OSctx1un9c3o602Ov7flPW/5xTUvSun+UtlYtNGrf6ekTvLXT/5aHaplTf4a72cAABxPhGcBAAAAAKAJe339696b31VhWr49+tty5TNXVv/8wj4XSma7TO/v47qOk5U7V5r7uWX0Lcdcl9S2qfLslc/KHW/c4cUtd8cb2H6gF4Pd+dvX/ya3v367zHxnpgxqP0jO7n529c8AADhe+JcFAAAAAIBT2MhOI+Xz/M8lf09+9eR3bVzolqO1bdm2+u8uvIpLKlpX3VO7y6biTTKu2zhv2b0d7tZZJvSc4P1x9h3cJx1/29GbOHdcQtBF/7mouuxps0/zJs8BADieCM8CAAAAAMAprE9GH7l84OVeMlAXi9xxSTX/99P/lfWF670Y4S7kScm+Eu9nLvzKBb0vOK51uHLglTL3o7ly6PAhLya6i3Hu3m637CjdUf33n7/1c/lSzy9556B/NnfZXC+UjPs5AADHE2+aAwAAAABwipt36Tz5xVu/kLGPj/XCmRyOHJZzup8jX+71ZflK36/Iqp2rZNwT46R5s+YyNHuoPHzRw3U+hotpvr10u9wz4R7fz64deq0Xg7zvg329N9mnnjFVhmQPqY5b7v48funj3rKLi/725re9N9rdm+lPXPpE9X4eW/aY/HnlnyUiETkt6zR5fvLzvjfjAQA4Vs0i7uNlAAAAAAAAAABAeBYAAAAAAAAAAKowaQ4AAAAAAAAAQBST5gAAAAAAAAAA1HfS/K233pJLLrlEOnfu7CXbeOGFFwK3Wbx4sYwcOVLatGkjffr0kaeeeqquhwWAUOijAMQz+igAAID64TkKQFxPmpeXl8uwYcNk9uzZocpv2LBBLr74YpkwYYKsWLFCfvCDH8i3vvUtefXVV+tTXwCoFX0UgHhGHwUAAFA/PEcBaEjNIpFIpN4bN2smzz//vFx22WU1lrn99tvl5ZdfllWrVlWv+/rXvy5FRUWyaNGi+h4aAALRRwGIZ/RRAAAA9cNzFIATreWJPsCSJUtk4sSJMesmTZrkfcJXk3379nl/qhw+fFgKCgokMzPT6xgBxCf3GVxpaan3dbnmzRtHygT6KKDpoI+ijwLiWWPsowAgnvEcBTQdkRPwHHXCJ81zc3MlOzs7Zp1bLikpkYqKCklISPBtM3PmTLn77rtPdNUAnCBbtmyRrl27SmNAHwU0PfRRAOJZY+qjGsrDDz/sW+f6waO5ia0gVplWrVrFLO/evduceNNatGhR634c3UeXlZX5yuzdu9e3LjExMWb50KFDvjIuPnMQXce2bdsGlqmpnJaamhqzfPDgQfPfsKDjWeevz9c6Vxem42jWF+itSZMDBw7ELLt/S4PuLauObhJV09fXuiduvPHGWpetOrZs6Z+2CXu+mr5OVh2HDx8upwqeo4CmZ8txfI464ZPm9XHHHXfI1KlTq5eLi4ule/fu3omnpKSc1LoBqJl7+OjWrZskJyfLqYw+Cmic6KPoo4B41lT6KACIZzxHAY1TyQl4jjrhk+YdO3aUvLy8mHVu2XU21qd6VZ8kW58mu23opID415i+tkYfBTQ99FEA4llj6qMAIJ7xHAU0Pc2O43PUCQ+WN27cOHnjjTdi1r322mveegA42eijAMQz+igAAID64TkKQIO+ae7idK1bt656ecOGDbJixQrJyMjwvrLivsqybds2+cMf/uD9/JZbbpGHHnpIfvzjH8s3v/lN+fvf/y5//etfvQzGAHC80UcBiGf0UQBQd9Yboe3atatz3GcrfrPez29/+1tfmffeey9wu6ysLF+ZDh061BoH3LG+Rm7Fqw4qY70Zq2O4W/u14qWHiamtY2NbMc2t2Og6PrgVU12v279/f2D8biuetxWL/OgEjzVx/yYHtVHr1q1967Zv3x54rKeeeipmedSoUb4yY8aMqTXGek2/E/q6Wfe7vpZhcgHEE56jADSkOr9p/uGHH8qIESO8P46L9eT+Pn36dG95x44dsnnz5uryPXv29Dok92nesGHDvIeQxx9/3MtYDADHG30UgHhGHwUAAFA/PEcBiOs3zc8777xaP/nWn5xWbbN8+fK61w4A6og+CkA8o48CAACoH56jADSkEx7THAAAAAAAAACAU/ZNcwAAAAAAGooVU1vHubbePg0Tr1m/gerCP2hWbOjMzMyY5c6dO/vK6DjjVtxtq476XHT8dKtNrLjfOu62FT/dOjcdn9xqf806N91GVpxxq94VFRW1xuq24p5bMdWteyIpKSlmuVmzZoF1tI5vtaW+TlbbFhQUxCy7eNvar371q8B4+VacdX0NrDK6TayY8gCASrxpDgAAAAAAAABAFJPmAAAAAAAAAABEMWkOAAAAAAAAAEAUk+YAAAAAAAAAAESRCBQAAAAAELfKy8sDExpaySqt5JTaK6+8ErO8Z88eXxkrEadOFmkllAyTZDFMAtN9+/b5yuh6JiQk+MroRJBW0lGdrNQ6X50809q3VceSkpLA87USeOrEm1a9ddtaZaxEnGHqra+ltR/rPklMTIxZLi4uDizz0Ucf+cq8+eabMcvXXHONr0xRUVHgvq020eus+xYAUIk3zQEAAAAAAAAAiGLSHAAAAAAAAACAKCbNAQAAAAAAAACIIqY5AAAAACBuWbHBdWxmHavaikW9efNmX5kVK1YExvi26Ljf1vF1bG4rfnSYmOYWHcO8bdu2gfux4odbMb11e1vtr+PMl5WV+cpYscD3799faxuFjcUeJl68dU3qs2/rGllSUlJilvPz831l9Placfefe+65mOUJEyb4ymRkZATu26q3Pp51TwAAKvGmOQAAAAAAAAAAUUyaAwAAAAAAAAAQxaQ5AAAAAAAAAABRTJoDAAAAAAAAABBFIlAAAAAAQKNmJYbUyTKXLVsWmAjTSqiZlJTkW5eYmBiY0FGvs8pYyUH1OquMTrJpJX1s3bp1rcvWfqy2tMro8w+zH6eioiJmubCw0FdGJ+e09h0m6aeV5FMf37ommtW21j2h62CV2bt3b+D99vnnn8csL1++3Ffm3//9333rioqKAttfn0uYhKoA0FTxpjkAAAAAAAAAAFFMmgMAAAAAAAAAEMWkOQAAAAAAAAAAUcQ0BwAAAADELStetY5FbcX9LigoCIwNrWM6WzGerZjabdq0qTV+ek370g4ePFivc9OxuZOTk+sV9/zAgQOBx7dig+vz18tW/G5r31ZMcV1va9/79+8PPDer3u3atYtZ3rNnT73qGEZOTk7g8XUccivO/v/93//5ypxzzjnH5XfJurcAAJV40xwAAAAAAAAAgCgmzQEAAAAAAAAAiGLSHAAAAAAAAACAKCbNAQAAAAAAAACIIhEoAAAAACBuWYkYdZJNq0x+fn7McmFhYWBiRCvpZFpamm9d69atY5ZbtmwZmIjSSlZpJWLUdUhKSgrczqp3VlZWYLLSMMlBwyRitZKlWvS+MzIyAq+tlSy1rKws8NxKSkp863Q9re10Ik69bO3HSs5q1VsnAtXbWNf2s88+85WxEpimpKTUmizVum9JBAoANeNNcwAAAAAAAAAAopg0BwAAAAAAAAAgiklzAAAAAAAAAACimDQHAAAAAAAAACCKRKAAAAAAgEZFJ1lMTEz0ldm8eXPM8t69e31lUlNTY5bbtm3rK2PtW6+zEjrqpI86MWhN9L508kwrWaZO8Ggd30p6aSWC1AlErWShuv2tRKRWnXT7Wuem620l4tT11olBrTJWAlMrWadOsmntOzMz07dOt0GY66bb2kpgaiU03b17t2+d3peVCNS6lgAAG2+aAwAAAAAAAAAQxaQ5AAAAAAAAAABRTJoDAAAAAAAAAHAsMc1nz54tv/nNbyQ3N1eGDRsmDz74oIwZM6bG8rNmzZJHHnnEiynn4mxdccUVMnPmTDNeHAAcK/ooAPGMPgoA6saKTa3jXKekpPjKFBcX17qNtV3nzp1DxTTXsaGTk5N9ZVw/f7SdO3f6ylhx1nVsbOv8s7Oza43N7nzyySe1buN06NDBt07H8O7evbuvTKtWrQJjZevY5Fa8divuud5XUVGRr8ymTZsC44eXl5f71h06dCiwjK6jFffcinOur4EV513Htdcx1p309PSY5V27dgXGXbfuE+u+0XHOrTj38Y7nKABx+6b5ggULZOrUqTJjxgz56KOPvE5q0qRJ5gOAM3/+fJk2bZpX/rPPPpMnnnjC28dPfvKT41F/AIhBHwUgntFHAQAA1A/PUQDietL8gQcekClTpsgNN9wgAwcOlDlz5nifIM+bN88s/95778lZZ50lV199teTk5MgFF1wgV111lSxduvR41B8AYtBHAYhn9FEAAAD1w3MUgLidNHdf5Vm2bJlMnDjxyA6aN/eWlyxZYm5z5plnettUdUrr16+XhQsXykUXXVTjcdzX5kpKSmL+AEAQ+igA8Yw+CgAAoH54jgIQ1zHNd+/e7cUA07HQ3PLq1avNbdwnem67s88+24tN5uKB3XLLLbV+HcbFl7r77rvrUjUAoI8CENfoowAAAOqH5ygAjSIRaF0sXrxY7r33Xnn44Ydl7Nixsm7dOrntttvk5z//udx5553mNnfccYcXp6qK+2SvW7duJ7qqAJog+igA8Yw+CgD8yRtrSqCo6QSSVmJEnayyZUv/ENlKMqkTOrokg0EJJbt06RK4HysRpnV8fS5Wskqd6DA/P99XxtpOv1lbWFjoK6MTploJPa11OoGmdfyCgoKY5bVr1wa2bVJSUuB+rGSwVgJTXe/27duHSsSZl5dXa7JUK8mndfyMjIyY5W3btvnKuDemtXHjxgXu20pOeirjOQpAg02au0zDLVq08P1j4JY7duxobuM6omuvvVa+9a1vectDhgzxMlTfdNNN8tOf/tR82HEdeVPrzAEcO/ooAPGMPgoAAKB+eI4CENcxzd0nrqNGjZI33ngj5pNxt6w/1Tz6E1jdEbmOrqZPPgGgvuijAMQz+igAAID64TkKQNyHZ3FfU7n++utl9OjRMmbMGJk1a5b3SZ3LXuxcd9113tfOXBwo55JLLvEyHI8YMaL66zDu0z63vqqzAoDjhT4KQDyjjwIAAKgfnqMAxPWk+eTJk2XXrl0yffp0yc3NleHDh8uiRYuqkzG4WG5Hf5L3s5/9zIu35v7vYnG5eGCug/rlL395fM8EAOijAMQ5+igAqDsr7reOPW7FxtZxp62Y5gkJCTHLVrgGKza13reOje7oOMg6DnhNdD3379/vK6PjfG/fvj0wprk+V8eaONTlrOO7f8tqi8NtxQ+3Yppb8br1dtZ+dExzHb++pjeJ9X3Ts2dPMwxIbceqad/6nvjss88C21vHOLfuwTCx4cPGudf3svW7Fc94jgLQkJpFGsF3UlzihdTUVCkuLpaUlJSTXR0ANWiqv6tN9byBxqap/q421fMGGht+V2vmkvgFTT5ak9ZPPvlkzPI//vEPX5l+/foFTv5a8Y31BKk1sZqZmdlgk+bWJGqYSXN3zwWdb3Jysq9MYmJiYLtZH0DUZ9J89+7dvjLu7eLatnGsqQ7dtg09aa7b0po019fb2s8tt9wSuM5NKmv6HrA+SHIhUHAEfTPQdH9X6xTTHAAAAAAAAACAUxmT5gAAAAAAAAAARDFpDgAAAAAAAABAfROBAgAAAADQUKzY2DoWs1VGJ320ytQ3WaZOoFiViLC2+NUHDhzwlbFiseuY1jp+uDNs2LCY5U2bNvnK6FjghYWFvjKHDh3yrdNxtq2413o7K6GkFQtexyIvKCgIVaega5uUlOQr0717d986lwgy6Nq6uLhBMc2tJJv6fK3j6xjuVkxzHS/dKmPdE7qe+t62NIIUdwBw0vCmOQAAAAAAAAAAUUyaAwAAAAAAAAAQxaQ5AAAAAAAAAABRTJoDAAAAAAAAABBFIlAAAAAAQNyyEnjqdVYiSp0s0kq62bp168DjWwk8deJJK+mi3s5KHmnVSSd+tJKT6nXdunXzldFJNlevXu0rs3PnTt+6LVu21JrQ1Ep8arW/lZxTJ6u02lYnME1JSfGV6d27d8xyTk5OqOPrRKRWks89e/YElrHOV2+3b9++wGtrJT3VyUmt+8ZqN10n6/j6Pm3VqpWvDACgEm+aAwAAAAAAAAAQxaQ5AAAAAAAAAABRTJoDAAAAAAAAABBFTHMAAAAAQKOiY0FbcZ/DxAbXMZ2tWNWRSMS3Tse5DhOv3Cqj4447iYmJMcsZGRm+Mm3atIlZLi0tDYzpreOA1xQvXsc0LysrC4yN3b17d18ZKxb5jh07AvetY4N37do18JpY8dqtuN9ZWVmB94S+TsXFxb4yJSUlvnX79+8P3HezZs1ilvfu3RvYtlZs9rS0tMD7RrejVUfrfgcAVOJNcwAAAAAAAAAAopg0BwAAAAAAAAAgiklzAAAAAAAAAACimDQHAAAAAAAAACCKRKAAAAAAgLhlJSvUiUCtxIj9+/ePWX733XcDE2Hq/da0TtfJSsRZVFQUmJgxMzPTt65jx461Jv20klVa+9YJPXWCyZqSTOqknno/ViLOli1bBiamtBJR6mUnNTU18Px1AlErMadOxOqUl5fHLG/fvj0wyWthYaGEodvESvKanZ0ds7xv377ABKbWddu2bZtv3dtvvx2z3K9fv8DEtwCAmvGmOQAAAAAAAAAAUUyaAwAAAAAAAAAQxaQ5AAAAAAAAAABRxDQHAAAAADSqmOYtWrSIWV6/fr2vzMaNGwNjXLdu3Tpmee/evYHxw61Y6FYd9b6s2NR5eXm+dbm5uYHx2nVMbx2H3IoFbp1bp06dfOt0fHIrfraOu52fnx/YRlacbysWe5cuXWKWO3fuHNhG1rGsc/vggw8CY3zrWPSDBw/2lbGO984778QsZ2Rk+Mo0a9as1vvPuk5WvPg//vGPvnUvvPBCzPJ//dd/+cqce+65tcZvBwAcwZvmAAAAAAAAAABEMWkOAAAAAAAAAEAUk+YAAAAAAAAAAEQxaQ4AAAAAAAAAQBSJQAEAAAAAcUsnT7SSc77yyiu+Mjo5pZVQ8dChQ4HHsrbTSUWthIrFxcW1Ji91zjvvPN+6lStXBiYLbd++fczy1Vdf7Sujt3vrrbcCE3NaiTetpJf79u2rNTGoU1JS4lunE6amp6f7yui2tPZTWloas3zGGWf4ykycONG3rmvXrjHLv//97wPLfPvb3/aVef31133r3n///VqvkZOYmBiYQFa3ZSQS8ZXJysryrdMJW//xj3/4yowbN67WYwEAjuBNcwAAAAAAAAAAopg0BwAAAAAAAAAgiklzAAAAAAAAAACiiGkOAAAAAIhbVizwHTt2xCx/9tlnvjJpaWmB8bP37t0bs5yQkBAY99yKfW7tu6ysLDA2d8eOHX3rOnXqFLP83HPP+cr06tUrZnnr1q2BbTRgwIDAOjqrV68OjNdu1Vtr3bq1b51uJx133ikqKopZ3rBhQ+D55+bm+so8+uijgfHaR44c6SujY7jPnTvXV2bz5s2+db179w6Mj9+mTZtaY7Nb97sVU163kRVnfv369b4y+h5MSUnxlQEAVOJNcwAAAAAAAAAAopg0BwAAAAAAAADgWCbNZ8+eLTk5OdK2bVsZO3asLF26tNby7qtD3/3ud72vmbmvI/Xr108WLlxYn0MDQCD6KADxjD4KAACgfniOAhC3Mc0XLFggU6dOlTlz5ngd1KxZs2TSpEmyZs0a6dChg6/8/v375fzzz/d+9uyzz0qXLl1k06ZNvvhyAHA80EcBiGf0UQAAAPXDcxSAuJ40f+CBB2TKlClyww03eMuus3r55Zdl3rx5Mm3aNF95t76goEDee+89adWqlbfOfSpYG5fA4ugkFlbCFACw0EcBiGf0UQBQdzp5ouP6xqPt3r07cD9W0sOWLWOHxOXl5b4yycnJvnWpqamByTK7du0as3zw4EFfmSVLlvjWJSYm1ppg0jl8+HDM8hdffOErk5GRUes2NSXiPHDgQMxyjx49As/fvfWrRSIR3zqdaFWfq7WdvtaOniC12qhdu3a+dVu2bJEgxcXFvonXoPO3En9aiVDDJALV96R13cK0m3Uv62ShjQ3PUQDiNjyL+8di2bJlMnHixCM7aN7cW7b+sXdefPFFGTdunPd1mOzsbBk8eLDce++9ZgbyKjNnzvT+Ear6061bt7pUE0ATRR8FIJ7RRwEAANQPz1EA4nrS3H167zoX19kczS3n5uaa26xfv977GozbzsWNuvPOO+W3v/2t/OIXv6jxOHfccYf36W7VnzCfBgMAfRSAeEYfBQAAUD88RwGI+/AsdeW+SuS+OvXYY49JixYtZNSoUbJt2zb5zW9+IzNmzDC3cV9Zsr6CBwDHG30UgHhGHwUAAFA/PEcBaLBJ86ysLK+jycvLi1nvljt27Ghu4zIUu9hRbrsqp512mvdJoPt6jRXnCwDqgz4KQDyjjwIAAKgfnqMAxPWkuetQ3Cdzb7zxhlx22WXVn9y55VtvvdXc5qyzzpL58+d75Vy8KWft2rVe50UHBeB4oo8CEM/oowCgfqwEmmVlZTHLVp9Y1W9WseIY6+SgVtLJ2uIf15Ys9OiJOivBplXGSvxoldHJOa1knYWFhTHLerKxpjrppJ5WQs0wST+rEi8ebefOnbVeI2s7K+mlTs5pXaP27dsHXqc9e/YEJgK1kqXqhKZh28RaF9T+e/fu9ZWx2kS3pU5MaiW11KFO4hnPUQDiOqa5M3XqVJk7d678/ve/l88++0y+/e1ve1mZq7IXX3fddV4MqCru5+7B47bbbvM6J5fZ2CVecIkYAOB4o48CEM/oowAAAOqH5ygAcR3TfPLkybJr1y6ZPn2695WW4cOHy6JFi6o/ody8eXPMJ5wu0/Crr74qP/zhD2Xo0KHSpUsXr8O6/fbbj++ZAAB9FIA4Rx8FAABQPzxHAWhIzSJhvh90krmvEKWmpnpfk9JfnwMQP5rq72pTPW+gsWmqv6tN9byBxobf1Zo98cQTvnVvv/12zPJf/vIXX5m0tLSYZReSQdOxkK3wLFYoDB36o2VL//toOvRHPIZncUkRtdLS0lrb0QqhYoVwcfdz0PGs9q6oqAgMT6LDivTr189Xxvo90vs62eFZrH3r622dv7VOt5t1T959990xy3379vWVGT16tG9dU0bfDDTd39U6v2kOAAAAAEBDseJe65jW1gBZx6+2Joj1ZK81Qd6mTRvfOh0v2prYrk9sbmuS2oqXrif7rZjeel1GRoavzO7duwMnZK067tu3T4JYMbV1G1iTxnpi2Wp//QGENfltHV/vW8cPt84tPT098IMNq92sSXMdi9+qows3EvSBhDVprn8HrA9pdFta+QIAAPWMaQ4AAAAAAAAAwKmKSXMAAAAAAAAAAKKYNAcAAAAAAAAAIIpJcwAAAAAAAAAAokgECgAAAACIW1ayRJ300EoEmpqaGrNcXFzsK6MTIerknTUlIq2oqAhMuqi3s5KFJiQkBCbwtBJK6gSaLVv6h/b6XHJzc31lrCSfmZmZMcsFBQW+MrpOrVu39pXp1KmTb11eXl5gm+g6WW2kE39a19ZqE71vq976nrDKWIlHdQLRMAlkrYSiOjmplazTum45OTm1trV1LlYCWQBAJd40BwAAAAAAAAAgiklzAAAAAAAAAACimDQHAAAAAAAAACCKmOYAAAAAgLhlxW9u165drfHLneTk5MB45TqmthU/2opfreNVW7G5dbxqK361tZ1WVlYWGC/dOjcd592KDW+1rY7PvnfvXl8ZfS5W/O7OnTsH7nvHjh2Bscita6LbzYp7brW3bgPr3HScb6uNrDjz+ppY9Q6qT9gyVpx5HcO8Q4cOgfHSrfMAAFTiTXMAAAAAAAAAAKKYNAcAAAAAAAAAIIpJcwAAAAAAAAAAopg0BwAAAAAAAAAgikSgAAAAAIC4VVFR4VuXlJRUa9JLK6GklSxUJ0LUSSDDJqLUx3L27dsXs1xaWhoqyaNOcpqWlhZYb2s/OjGllfRSJ4a0kmpaSUb1vocMGeIrM3jwYN+68vLymOVOnToFJgu16GtiXX+LPr6VLFTfb9Y9Ya3TrGuij2cl4tTb6bauaV1JSUnM8oABAwKvbZhkpQDQVPGmOQAAAAAAAAAAUUyaAwAAAAAAAAAQxaQ5AAAAAAAAAABRTJoDAAAAAAAAABBFIlAAAAAAQNyykh62adOm1sSgzvbt22tN3mklubQSM4ZJllhUVBSYLLJ169ahzk0nFbUSoepzsZJ17ty5M3A/VrvptrWSZYZJFrpjxw7fOp34c+PGjb4yycnJgQk19flv3rzZV2bQoEG+dToZqnVt9fH2798fWMbal3Uv6WtbXFzsK6Pb0krgqtvISjyblZUV2G7WuQEAKvGmOQAAAAAAAAAAUUyaAwAAAAAAAAAQxaQ5AAAAAAAAAABRxDQHAAAAAMQtK350YmJirfGcrVjcVkxvHVPcivFtHV/HK7fifut9HzhwIHA/YWOa6zqFqffWrVt9ZTZs2OBbp2Nxd+7c2VdGx8tesWKFr0zbtm0Dzy03NzewTmVlZb4yur3bt2/vK9OlS5fAeO3WvjUrNrl1vfX1teKl65jiVpx9HcM8TEx3q1yvXr0Cy1jnAQCoxJvmAAAAAAAAAABEMWkOAAAAAAAAAEAUk+YAAAAAAAAAAEQxaQ4AAAAAAAAAQBSJQAEAAAAAcctKlpmQkBCznJmZ6Sujk4OGSboYJqGntV3r1q0DE0haCSWtdfp8rfO3ziUo6aN1Hh06dPCt27hxY8zy8uXLfWWys7MD62ita9WqVczyli1bfGXWrl0bmAhTJ/7cv3+/r0xhYaFvXdeuXQOTs1r7Crr+NZ1v0HbWfqwEqtru3bt96wYNGhSzPGDAAF+Z/Pz8Wq8HAOAI3jQHAAAAAAAAACCKSXMAAAAAAAAAAKKYNAcAAAAAAAAA4Fhims+ePVt+85vfSG5urgwbNkwefPBBGTNmTOB2Tz/9tFx11VXy1a9+VV544YX6HBoAAtFHAYhn9FEAUDdW3G8di7pPnz6+MqtXr65zvPKDBw+GijGt62Rtp+NVt2vXzlemTZs2gdtZMb1TUlICY1Pr2NylpaW+MlYs+KFDh8Ysf/HFF74y27dvj1letWqVr0xycnJgLHar3fr37x8Yd11fN/dvapi433pf1r1VXl5e67Fqinuur5N1TXTcc+vestokjNNOOy1muWVL/3SPdS6NDc9RABpKnXvMBQsWyNSpU2XGjBny0UcfeZ3UpEmTZOfOnbVu55KJ/OhHP5Lx48cfS30BoFb0UQDiGX0UAABA/fAcBSCuJ80feOABmTJlitxwww0ycOBAmTNnjveJ+bx582rcxn2aes0118jdd98tvXr1OtY6A0CN6KMAxDP6KAAAgPrhOQpA3E6au68gLVu2TCZOnHhkB82be8tLliypcbt77rnH+xrUjTfeGOo4+/btk5KSkpg/ABCEPgpAPKOPAgAAqB+eowDE9aS5iwnmPqXLzs6OWe+WrRhizjvvvCNPPPGEzJ07N/RxZs6cKampqdV/unXrVpdqAmii6KMAxDP6KAAAgPrhOQpAo0gEGpZLNHLttdd6HVRWVlbo7e644w4vTlUV98keHRWA440+CkA8o48CgPCJETt27Ohb17t375jlTz75xFcmISEh8Fg6MWdNCTw1K8lkfZKc6uSZVkLHPXv2+Mps3bo1ZllPNjo7duzwrdNtYLVtRkZGzHJeXp6vjPWGrj5fNykZlEDUSrqpz83aj5X4VU+uWmXcm8ZBCTWtaxKU9NO6bvpaW6x78rzzzvOtO//88wO30+drJZk9VfAcBaBBJ81dR+P+cdD/ILpl6x9Sl2XbJVy45JJLfJ20+4dnzZo1vgeZqgeQMA8hAHA0+igA8Yw+CgAAoH54jgIQ1+FZWrduLaNGjZI33ngjptNxy+PGjfOVHzBggKxcuVJWrFhR/efSSy+VCRMmeH/n0zoAxxN9FIB4Rh8FAABQPzxHAYj78CzuayrXX3+9jB49WsaMGSOzZs2S8vJyL3uxc91110mXLl28OFBt27aVwYMHx2yflpbm/V+vB4DjgT4KQDyjjwIAAKgfnqMAxPWk+eTJk2XXrl0yffp0Lx7Y8OHDZdGiRdXx0TZv3uyL0wUADYU+CkA8o48CgLoL0y/qONTOoEGDYpatZIE67rZ1LCs2tY5z7t6C1dykXV3jZ1sxtMPUyTo3HdM6KSnJV6Z///6+dbqe+fn5vjI9evSota1riteuz2X9+vW+MvpcdDs6OTk5Mcvp6em+MlZM74KCgsB96/O3Yqq3atXKty7M8fU6K156mBj+F154YWAseCteur4mYc4jnvAcBaAhNYuEyU5ykrkHGZfYo7i4WFJSUk52dQDUoKn+rjbV8wYam6b6u9pUzxtobPhdrdkDDzzgW6cnxqyJbdeWRzs6rENdJs3DrLMmzTUr6aK1Tk+k6mSl1iSxNbFbUVEROEFsnVt9Js07dep0wibNraSbemI57KS5TphqtYl1vidq0tz6IEWXsfYzbdo037ru3bsHTprr41n37Ze//GXfuqaMvhlour+rfAQHAAAAAAAAAEAUk+YAAAAAAAAAAEQxaQ4AAAAAAAAAQH0TgQIAAAAA0FCsuM9hUnPpONdjxozxlXnzzTcD92vFS9esuNM6XrS1H+vcdLxsK155aWlpnROKWnW0ElHqelttouOO9+nTx1fGiqmtz61du3a+Mtu3b681VreTmJgYs7x7925fGSsWepg20edrnYfFasugBLJhkrympaX5ylixyPW+reum28SKqQ8AqMSb5gAAAAAAAAAARDFpDgAAAAAAAABAFJPmAAAAAAAAAABEMWkOAAAAAAAAAEAUiUABAAAAAHHLSqCpExhaSR91Asfs7GxfmczMzFqTUDopKSmBCS2tOup1Vh3DJGK0ypSXlwcmfdTJIq2kl2GSjFZUVAS2W2Fhoa+MtU4n/rSSbCYkJMQsl5SUBCZQbdOmja+MtW/dBlYiTqudNCvxqtWWQclCrbbV90379u19ZZKSkgLrFObeshLRAgAq8aY5AAAAAAAAAABRTJoDAAAAAAAAABDFpDkAAAAAAAAAAFHENAcAAAAAxC0dB9qKO23FFNcxra2Y4snJybVuU9M6HS/aqqMuY9XRikWuy1nxs3Us7rZt2wbW24qfnZiYGBgvu0ePHoEx3a3Y4N27dw+Ml25dk2HDhgXGRi8uLo5ZLioq8pWxrok+XysWuo5NHqb9rXV79+71ldmzZ09g3HF9vKysLF8Zq946zr1F329h4rcDQFPFm+YAAAAAAAAAAEQxaQ4AAAAAAAAAQBST5gAAAAAAAAAARDFpDgAAAAAAAABAFIlAAQAAAABxy0qWqdeFSbJ5+PBhX5lOnTrFLK9ZsyZU0kcrgWUQK+mjtR99LlYi0pSUlMCEjjqBZk5Ojq9M69atA5NlWgk19fF18sywyTmt4+tkmVay0oSEhFoTkzplZWWB+7ZY90mYe1JvF+aetBK46nWDBw8OdXx9n1j71vegdW8BACrxpjkAAAAAAAAAAFFMmgMAAAAAAAAAEMWkOQAAAAAAAAAAUcQ0BwAAAADELSt+s44zbsX01vGzrXjWGRkZMctJSUm+Mta+NSumt44fbcU0t9bpc7POX29nxa/W52/F6i4vL5cgbdq08a3TsbCt87Bieuu2tGJq61jkVkz14uLiwGPpuOdOQUFBrfux2tuKu261pVWHoLa0Ytp37949ZrlPnz6hrpuud5i461a8fgBAJXpIAAAAAAAAAACimDQHAAAAAAAAACCKSXMAAAAAAAAAAKKYNAcAAAAAAAAAIIpEoAAAAACAuGUlmQyT9FAnZrSSTuokk+np6b4yGzZsqFciUp3k0UoeaZ2bTmBpJZgsLCyMWW7VqpWvzL59+2KWd+7c6SuTmJgYuG+rTXRCSytZqEVfA+vc9LXMy8vzldEJPK3EmDqhqNOjR4+Y5dLS0sBrayVZDZPk1Cqj7wHrnujbt29gItSKiorA+82633WdrOsPAKjEm+YAAAAAAAAAAEQxaQ4AAAAAAAAAQBST5gAAAAAAAAAARDFpDgAAAAAAAABAFIlAAQAAAABxy0qouH///jqX0YkSraSTnTp18pVZu3Zt4HaWMMlKrXVhkmXqpJNWskid+NNKaGkl+Wzfvn3McteuXX1lBg4cWGvyUquOVsLQPXv2+Mps2rQpZnnVqlWB+7aSXlpJLnV7W4lYdSJQK1mnvres62RdN72udevWvjJ9+vQJbMf63m/6PglzHwNAU8Wb5gAAAAAAAAAAHMuk+ezZsyUnJ8f7pHrs2LGydOnSGsvOnTtXxo8f732C7f5MnDix1vIAcKzoowDEM/ooAACA+uE5CkDcTpovWLBApk6dKjNmzJCPPvpIhg0bJpMmTfJ99avK4sWL5aqrrpI333xTlixZIt26dZMLLrhAtm3bdjzqDwAx6KMAxDP6KAAAgPrhOQpAQ2oWqWMQK/dJ3umnny4PPfRQdXwv1/F873vfk2nTpgVu72J4uU/43PbXXXddqGOWlJRIamqqFBcXS0pKSl2qC6ABxcPvKn0UgHj+XaWPAlATfldrdu+99/rW6RjWVtxpHQvaimmu91NWVuYr88orr/jW6VjUVkxxHS9bx8q2yjh6iG7F69YxxJOTk31ldJ1atWrlK2Ot07HP3X2pde/ePTA2uhX3W1+DvLw8X5kNGzbELLvfiaB9l5eXhzr+vn37AuOF62tiTZno/Vjbhdm3FdP8+9//fsyye07QrPtU79u6J/XxrHvykksukXjFcxSAhvxdrdOb5u4fnWXLlnlfaaneQfPm3rL71C4Ml+jD/aOfkZFRYxn3D5A72aP/AEAQ+igA8Yw+CgAAoH54jgLQ0Oo0ab57927vk7ns7OyY9W45Nzc31D5uv/126dy5c0xHp82cOdP7dKDqj/XJKgBo9FEA4hl9FAAAQP3wHAWgUSQCra/77rtPnn76aXn++ed9X/k62h133OG9Tl/1Z8uWLQ1ZTQBNFH0UgHhGHwUAAFA/PEcBqCt/kKtaZGVleTHIdNwxt9yxY8dat73//vu9Tur111+XoUOH1lq2TZs23h8AqAv6KADxjD4KAACgfniOAhDXk+YuacSoUaPkjTfekMsuu6w68YJbvvXWW2vc7te//rX88pe/lFdffVVGjx597LUGAAN9FIB4Rh8FAMePTmAZJumklZgxTEJRy86dO2OWMzMzA+tosZKT6sSTViJKnfTROpY+N+v8rXjNesKwsLAwcDsr6ZqVwFS3b2lpqa/M3r17Y5YLCgp8ZfS5hEnMGTaBq96XdR7WvvW1dKFENH08a99bt26NWe7atauvjHVP6H1bdQyTHDde8RwFIK4nzZ2pU6fK9ddf73U2Y8aMkVmzZnmZqm+44Qbv5y4DcZcuXbw4UM6vfvUrmT59usyfP19ycnKqY00lJSV5fwDgeKKPAhDP6KMAAADqh+coAHE9aT558mTZtWuX1/G4Dmf48OGyaNGi6mQMmzdv9jIYV3nkkUe8T/2vuOKKmP3MmDFD7rrrruNxDgBQjT4KQDyjjwIAAKgfnqMANKRmEet7PXHGffXLZS12SRisr30BiA9N9Xe1qZ430Ng01d/VpnreQGPD72rN7r333sAyVngWHQrECk+it6uoqPCVcWEdtKKiojqHZ7FCYVjrdL2tEB6JiYkxy+np6b4y1nZhyujwLFYdMzIyGmV4Fn1NrHA8el/WvWVNo+h2srbTIVSsEC5uYvhoZ5xxhq9MWVlZ4LkdPXlcU5tY17Yq9Akq0TcDTfd3tc5vmgMAAAAA0FCsyU89sWmV0ZOG1qS5njS0JiPz8/MDJz+tiV09kW4dv127doF1siaf9fGtyV+9H2ti2ZrY1ee7Z88eXxk92WvVMcykrVVG19sqo2OTW4kbw8TrtmK6W3HONatN9DVo27Zt4PGs6//CCy/ELA8cONBXxtp3UH2strTaFgBQiR4SAAAAAAAAAIAoJs0BAAAAAAAAAIhi0hwAAAAAAAAAgCgmzQEAAAAAAAAAiCIRKAAAAAAgbkUikcBkjVaSyzCJIfW+169f7ytj7VtvpxNjOhUVFTHLrVq18pXZu3dv4L6tBKI6EaVOjGkl+bSSflrbJSQkBCZH1QktrYSalqSkpFqPZV1bK1mmTihaWlrqK2MlJ9XJMa1EmPp6W9fWShaq9xUmEad1b+mkstY9OWjQoMDztZKFWucCALDxpjkAAAAAAAAAAFFMmgMAAAAAAAAAEMWkOQAAAAAAAAAAUcQ0BwAAAADELSsWuI4FrWNcW7HAdYxxJyUlJfD4VixwHXfcik2u6xTmWNa+rTjUOoa3FdNbx3C3YmxbdNta7damTZuY5fT09FDtpmOa6/1Ysbmt89dtpGO8W2WsdVbcb33drLjn1r71PWCdv76Xrf3oc9m+fbuvzIgRI3zr9PHCXDcrzj8AoBJvmgMAAAAAAAAAEMWkOQAAAAAAAAAAUUyaAwAAAAAAAAAQxaQ5AAAAAAAAAABRJAIFAAAAAMStMIkgrSSXuoyVdFInC7X2YyVL1MkhrUSkOjGkldDUovdl7Vsf30pWqc8tzH6cTp06xSxnZWX5yiQnJwcmtGzZsmVgkk+9bCWw1OdhJSstLCwMPFbYRJyadU8UFBQEXu8w9bb2rbez9mO1bZj7i0SgABAeb5oDAAAAAAAAABDFpDkAAAAAAAAAAFFMmgMAAAAAAAAAEMWkOQAAAAAAAAAAUSQCBQAAAADELSuho2YluQwjTCJQK1mmXpeQkOArs3///pjlsrKyUOfWtm3bmOXExERfmZSUlMB663U6eWdNiSAzMzNrTQzqpKWlBdZRn39N9dR0Us+tW7cGJt20krxa6/R100lHrX3r5J3Onj176nUP6u2s5J26jlab6Tpax2/dunW9EpECACrxpjkAAAAAAAAAAFFMmgMAAAAAAAAAEMWkOQAAAAAAAAAAUcQ0BwAAAAA0KpFIJDB+s44NbcWm1nGgw8Y01+WsMrqOVozvQ4cO+dYF1dE6XyumeJgY39bxdb2tuOslJSUxy6WlpYHx4q06WNdN18lqtzCx6K145bpceXl54LlZx9dtZMWHt9pWX8uWLVsGlsnPz/eVCbNvq966/eubCwAAmgLeNAcAAAAAAAAAIIpJcwAAAAAAAAAAopg0BwAAAAAAAAAgiklzAAAAAAAAAACiSAQKAAAAAIhbVtLFMGV0kkUrMeL69etjlnft2hUqgaZORBnm+BYroaNOWGoltNQJLJOTk31ldHJQK1mpdXydLHPPnj2+MjqBp5Vk1Upg2bFjx5jltLQ0Xxl9nYqLiwOTdVrnYbWbPhfrntDbhU0Oq+8Jq0zbtm0DE3EmJCTELH/wwQe+Munp6b51559/fuA9oZO6tmrVylcGAFCJN80BAAAAAAAAAIhi0hwAAAAAAAAAgCgmzQEAAAAAAAAAiCKmOQAAAAAgbun42VYMbSt+eFlZWczymjVrfGWWLVtW5xjfVgxzq4yut1VHK154mH3r2NRFRUW+MvpcwsSGt+KFW/HKU1NTY5ZbtGjhK7N169bAeOHWtdX1tNpI78eKH753717fOutcNF0naz9WLHJ9fa144VY7BV1ba5u//e1vvnXbt2+PWT7jjDN8ZXr16lXrdQQAHOOb5rNnz5acnBwvicXYsWNl6dKltZZ/5plnZMCAAV75IUOGyMKFC+tzWAAIhT4KQDyjjwIAAKgfnqMAxO2k+YIFC2Tq1KkyY8YM+eijj2TYsGEyadIk2blzp1n+vffek6uuukpuvPFGWb58uVx22WXen1WrVh2P+gNADPooAPGMPgoAAKB+eI4C0JCaRcJ+RyvKfZJ3+umny0MPPVT9VbFu3brJ9773PZk2bZqv/OTJk6W8vFxeeumlmK8JDR8+XObMmRP662Hua0PFxcWSkpJSl+oCaEDx8LtKHwUgnn9X6aMA1ITf1Zrdd999JzU8i96PVc4KoaLDalhlrNAj+lysOukyVniS5OTkWpfDOp7hWdq3bx/34Vl0e1v7sc5Xh2ypb3gWfZ+4N6SD6ui454vjEZ7lP/7jPyRe8RwFoCF/V+sU03z//v3eQ8Udd9wR84/TxIkTZcmSJeY2br37JPBo7pPAF154ocbjuH/Ijv7HzJ2wFVsNQHyp+h2t42dxxw19FIDa0EfRRwHx7GT3UQAQz3iOAtDQz1F1mjTfvXu394lmdnZ2zHq3vHr1anOb3Nxcs7xbX5OZM2fK3Xff7VvvPkEEEP/y8/NPSlIZ+igAYdBHAYhnJ6uPimfWG6QAmhaeowA09HNUnSbNG4r75PDoTwNdJvAePXrI5s2bG90DpPukw3WuW7ZsaXRf5aHuJ0djrrv7FL579+6SkZEhpzL6qPhA3U+Oxlx3+ij6qIZE3U+Oxlz3ptJHAUA84zkqPlD3k6Mx1734BDxH1WnSPCsry4vBlZeXF7PeLXfs2NHcxq2vS3mnTZs23h/NdVCN7aJVcfWm7g2Pup8cVkzBhkAf1TTvN+p+cjTmutNHNT6N+X6j7idHY677yeqjACCe8RzVNP9NpO4nR2Oue/Pj+BxVpz25JB2jRo2SN954o3qdS7zglseNG2du49YfXd557bXXaiwPAPVFHwUgntFHAQAA1A/PUQAaWp3Ds7ivqVx//fUyevRoGTNmjMyaNcvLRnzDDTd4P7/uuuukS5cuXhwo57bbbpNzzz1Xfvvb38rFF18sTz/9tHz44Yfy2GOPHf+zAdDk0UcBiGf0UQAAAPXDcxSAuJ40nzx5suzatUumT5/uJU8YPny4LFq0qDq5govzdPSr8GeeeabMnz9ffvazn8lPfvIT6du3r5epePDgwaGP6b4aM2PGDPMrMvGOup8c1L3p1p0+qm6o+8lB3Ztu3emj6oa6nxzU/eRozHUHgIbAc1TdUPeTg7qfOnVvFolEIsdtbwAAAAAAAAAANGJkmQEAAAAAAAAAIIpJcwAAAAAAAAAAopg0BwAAAAAAAAAgiklzAAAAAAAAAADibdJ89uzZkpOTI23btpWxY8fK0qVLay3/zDPPyIABA7zyQ4YMkYULF0pjqPvcuXNl/Pjxkp6e7v2ZOHFi4LnGU7tXefrpp6VZs2Zy2WWXSWOpe1FRkXz3u9+VTp06edl0+/Xrd9Lum7rWfdasWdK/f39JSEiQbt26yQ9/+EPZu3evNKS33npLLrnkEuncubN37V3W8SCLFy+WkSNHeu3dp08feeqpp6Sxoo86Oeij6KPCoo+ijzoZ6KPoo8Jq6n0UAMQznqNODp6jeI6K++eoSBx4+umnI61bt47Mmzcv8sknn0SmTJkSSUtLi+Tl5Znl33333UiLFi0iv/71ryOffvpp5Gc/+1mkVatWkZUrV8Z93a+++urI7NmzI8uXL4989tlnkW984xuR1NTUyNatW+O+7lU2bNgQ6dKlS2T8+PGRr371q5GToa5137dvX2T06NGRiy66KPLOO+9457B48eLIihUr4r7uf/7znyNt2rTx/u/q/eqrr0Y6deoU+eEPf9ig9V64cGHkpz/9aeS5556LuK7j+eefr7X8+vXrI+3atYtMnTrV+z198MEHvd/bRYsWRRob+ij6qLqij6KPakj0UfRRdUUfRR8FAKjEcxTPUXXFc1SnJvMcFReT5mPGjIl897vfrV4+dOhQpHPnzpGZM2ea5b/2ta9FLr744ph1Y8eOjdx8882ReK+7dvDgwUhycnLk97//faQx1N3V98wzz4w8/vjjkeuvv/6kdVJ1rfsjjzwS6dWrV2T//v2Rk62udXdlv/SlL8Wsc7/4Z511VuRkCdNJ/fjHP44MGjQoZt3kyZMjkyZNijQ29FH0UXVFH0Uf1ZDoo+ij6oo+ij4KAFCJ5yieo+qK56ipTeY56qSHZ9m/f78sW7bM+1pIlebNm3vLS5YsMbdx648u70yaNKnG8vFUd23Pnj1y4MABycjIkMZQ93vuuUc6dOggN954o5ws9an7iy++KOPGjfO+DpOdnS2DBw+We++9Vw4dOhT3dT/zzDO9baq+MrN+/XrvazwXXXSRxLN4+T09VvRR9FF1RR9FH9WQ6KPoo+qKPoo+CgBQieconqPqiueo9U3qOaqlnGS7d+/2bhR34xzNLa9evdrcJjc31yzv1sd73bXbb7/di8mjL2Y81v2dd96RJ554QlasWCEnU33q7n6x//73v8s111zj/YKvW7dOvvOd73j/QMyYMSOu63711Vd725199tnumyFy8OBBueWWW+QnP/mJxLOafk9LSkqkoqLCi4fVGNBH0UfVFX0UfVRDoo+ij6or+ij6KABAJZ6jeI6qK56jDjap56iT/qZ5U3bfffd5CQyef/55LwB/PCstLZVrr73WSxyRlZUljc3hw4e9TyQfe+wxGTVqlEyePFl++tOfypw5cyTeueQF7lPIhx9+WD766CN57rnn5OWXX5af//znJ7tqOMXRRzUc+iig7uijGg59FAAApxaeoxoOz1GN10l/09zd8C1atJC8vLyY9W65Y8eO5jZufV3Kx1Pdq9x///1eJ/X666/L0KFDpaHVte5ffPGFbNy40ctWe/QvvtOyZUtZs2aN9O7dO27b3WUobtWqlbddldNOO8379Ml9RaV169YSr3W/8847vX8gvvWtb3nLLjt3eXm53HTTTV5H675OE49q+j1NSUlpVG9H0UfRR9UVfRR9VEOij6KPqiv6KPooAEAlnqN4jqornqOGNKnnqJN+du7mcJ+0vPHGGzE3v1t2MX8sbv3R5Z3XXnutxvLxVHfn17/+tfepzKJFi2T06NFyMtS17gMGDJCVK1d6X4Wp+nPppZfKhAkTvL9369YtbuvunHXWWd5XYKo6Vmft2rVe59VQHVR96+7ijOmOqKqzrcyBEJ/i5ff0WNFH0Ued6Lo79FENL15+T48VfRR91Imuu0Mf1fDi5fcUAE5lPEfxHHWi6+7wHNXwjtvvaSQOPP3005E2bdpEnnrqqcinn34auemmmyJpaWmR3Nxc7+fXXnttZNq0adXl33333UjLli0j999/f+Szzz6LzJgxI9KqVavIypUr477u9913X6R169aRZ599NrJjx47qP6WlpXFfd+1kZiuua903b97sZYW+9dZbI2vWrIm89NJLkQ4dOkR+8YtfxH3d3f3t6v6Xv/wlsn79+sj//d//RXr37u1l7W5I7h5dvny598d1HQ888ID3902bNnk/d3V2da/i6tquXbvIf//3f3u/p7Nnz460aNEismjRokhjQx9FH1VX9FH0UQ2JPoo+qq7oo+ijAACVeI7iOaqueI7q3WSeo+Ji0tx58MEHI927d/d+gceMGRN5//33q3927rnner8QR/vrX/8a6devn1d+0KBBkZdffjnSGOreo0cP7wLrP+5GjPe6x1MnVZ+6v/fee5GxY8d6HUSvXr0iv/zlLyMHDx6M+7ofOHAgctddd3kdU9u2bSPdunWLfOc734kUFhY2aJ3ffPNN896tqqv7v6u73mb48OHeebo2f/LJJyONFX0UfVRd0UfRRzUk+ij6qLqij6KPAgBU4jmK56i64jmqsEk8RzVz/zm+L8EDAAAAAAAAANA4nfSY5gAAAAAAAAAAxAsmzQEAAAAAAAAAiGLSHAAAAAAAAACAKCbNAQAAAAAAAACIYtIcAAAAAAAAAIAoJs0BAAAAAAAAAIhi0hwAAAAAAAAAgCgmzQEAAAAAAAAAiGLSHAAAAAAAAACAKCbNAQAAAAAAAACIYtIcAAAAAAAAAIAoJs0BAAAAAAAAAIhqWfUXAADixfA5w73/7z+0X9bkr5EhHYZ4y/2z+suCKxac0GNvLNroHb9oWtEJPQ4AAAAANFnDK8d8sn+/yJo1IkMqx3zSv7/IghM75gPCYNIcABB3VtyyImYCu2r5aAcPH5SWzflnDAAAAAAanRXRMd7GjZUT6FXLRzt4UKRlIx3zNea6w0N4FgBAo5EzK0duf+12GTN3jFz/wvWyeOPi6rfSnVU7V3llqry67lU5e97ZMuqxUd42b254M/SxZrw5w9uuz+/6yMLPF1avv+a5a2T0Y6Nl6CND5eL5F0tuWW71BH/afWnyo//7kfezQQ8PktfXvx74s1sX3ir3vn1v9f7X7F4j3f6nm/ehAAAAAAA0KTk5IrffLjJmjMj114ssXnzkrXRn1arKMlVefVXk7LNFRo2q3ObNN8NNaE+aJDJ6tMigQSJXXy1SXl75M3e8wYNFvvMdkWHDKn/+4YdHtn30UZF+/URGjhT5+c9FmjU78jP39xkzRE4/XeSOOyrfnn/vvSM/f+wxkcmTj6190GCYNAcANCr5Ffnyz2/9U/78H3+utdz6wvVy1z/ukoXXLJRlNy2T+ZfPl6ufu1r2Hdzn/dxNtm8v3W5uW7yvWIZmD/W2e+iih+SHr/6w+mezJs2SD2/6UD7+9scyvvt4uWvxXTHbnZZ1mvezJy59Qq7+36uldF9prT/73pjvyWPLHpNDhw955R7+4GG5aeRNvEUPAAAAoGnKzxf55z9F/lz7mE/Wrxe56y6RhQtFli0TmT+/cgJ8X+WYz5ts326M+Vq0qCzrJsPdJHxqqsiDDx75+erVlRP2//qXyPe+J/LTn1aud2Xd8d56S+Sjjyon3619f/CByG9+I/L974s89NCRn82eLXLrrfVrEzQ4RuQAgEblG8O/Ic2O/jS/BovWLZJ1BevknCfPqV7XvFlz2Vy8Wfpm9jVDvlRp27Kt/Mdp/+H9fVzXcfJFwRfVP5u/cr788eM/yt6De70/We2yqn/mJrpd/Zwzup4hnZM7y/Lc5dI9tXuNPzunxzkysP1A+duav8mk3pPkL6v+Iiu/vbKerQMAAAAAjdw3vhH7BndNFi0SWbdO5JwjYz5p3lxk82aRvn3tkC9OJCLyP/8j8vLLlRPfxcUiZ5555Od9+oiMHVv593HjRO6/v/Lvf/+7yIUXinTsWLk8ZYrIPffE7vub3zzy9//8T5Hp00Xy8kQ+/7zynMaPD9sKOMmYNAcANCpJrZOq/+4mog9FKt/QdtwkdpVIJCLn9zrfe8O8rtq0aFM9Md+ieYvqY7yz+R353dLfyZIbl0iHxA7y4poXZfqb02vdVzNpFviz28beJr9691eyq3yXnN/7fMlOyq5znQEAAADglJB0ZMznxQU/dGTMJ3v3xk5+n39+5VvjdeHKuwnwf/xDJCVF5He/q1yu0rZt7Jvj1hvljjWxf3TdExIqPwBwIV0++0zku9+tWz1xUhGeBQDQaPVK7yWbijZ5k83OH//1x+qfTeozyYsb/nHex9Xrlm5bekzHK6wolOTWyZKZkCn7D+2XR5c9GvNzF4fcvYVedSwX/mV4x+GBP7ug9wVebPRfvP0LufV0vq4HAAAAAJ5evUQ2bRLZVTnmkz8eGfN5cclff13k4yNjPlkaYsxXWCiSlVU5YV5aKvLUU+HqMmFCZQz1nTsrl594IngbN1HuYpm7Sflrrgl3HMQF3jQHADRaLsTJj8/6sYx5fIxkJ2bLV/p8pfpnfTL6eG+Z3/zSzbLnwB5vkntExxHVb567mOYu3rnbR1gX9rlQ/rTyT9L/of6S2S5TJvacKNtKtlX/PLVNqpeMdNicYd4kuTtWcptkLw57TT9z3FvtN464Ueavmi/juo07rm0EAAAAAI1W584iP/5xZZLP7GyRr3wlNoyKe2v85ptF9uwR2b9fZMSII2+eu5jmLt6528fRrrtO5G9/E+nfX6R9+8qQKW5iPohL7Pmzn4mcdZZIcnJlqBYXD702XbtW1sklD23Xrj4tgJOkWcR9fx0AAByTjUUbvYn4omlFdfpZlX+b/28yedBkuXbYtSe4pgAAAACAenFvprsJc+f//b/KuOqvvFJz+fLyysn5t98W6dmzwaqJY0d4FgAATqIPt38ofX7Xx0tSevWQq092dQAAAAAANZk2rfIN9kGDRF56SWT27JrLzpkjMmCAyHe+w4R5I8Sb5gAAAAAAAAAARPGmOQAAAAAAAAAAUUyaAwDiUs6sHC/hposFPnD2QJm9tJavvYXkEnG6/YbR7O5mUrS35hjkAAAAAICQcnIqY3u70CYDB9Ye1iSsVasq93uyzJolkptb/+2bNRMpYswZr1qe7AoAAFCTBVcskOEdh8umok0ydM5QGd9jvAzNHlr988ORw97/XTxwAAAAAEAcW7CgctJ80yaRoUNFxo+v/H+Vw5XjO2neSMZ3btL8vPNEOnb0/6yxnQt8uHIAgLjXI62H9M/sL2vz18pdi++Sy/96uUz60yQZ/PBg2VG6Q15d96qcPe9sGfXYKBkzd4y8ueHN6m1d+b4P9vV+9vSqp+t03Ic/eNjbX8//11OeXP5k9fof/d+P5PS5p3tvwZ/z5DmyZveamDfUf/b3n8mIR0dIvwf7yZ8//nPgz+5/73656f+7qbqce8M969dZUlBRUO82AwAAAIC41KNH5Vvna9eK3HWXyOWXi0yaJDJ4sMiOHSKvvipy9tkio0aJjBkj8uaR8Z1Xvm/fyp89/XTd3uq+997K/bmknE8eGd/J55+LXHyxyOmnV07iP/RQzW+DZ2WJbNwocs89Itu3i0yeXPlBwIoV9rn86EeV+3VlzjlHZM2RsSPiG5PmAIC4tzJvpazevVqGZQ/zlpdsWSJ/uOwP8ul3P5V9h/bJXf+4SxZes1CW3bRM5l8+X65+7mrZd3CfvLz2ZXnm02e89R9O+VA2Fm2M2e9Ff75IPtz+YY3HbdOijSydslReueYV+f6i78vBwwe99befdbt8MOUDWXHLCvnO6d+R2xbdFrNdM2kmy29eLov+c5F875XvxRzX+tm3Rn5LXlj9QnU4GDdB/9X+X5WMhIzj2o4AAAAAcNKtXCmyerXIsMrxnSxZIvKHP4h8+qnIvn2Vk88LF4osWyYyf77I1VdXrn/5ZZFnnqlc/+GHlZPXR7voosr1NWnTRmTpUpFXXhH5/vdFDh4UOXRI5KqrRH77W5EPPhB5/32Rxx6r/Httpk8X6dy58u15N2HuJsX1uXTpInL77ZX7cmW+8x2R22LHjohfhGcBAMStyc9OloSWCdKuVTuZ99V50jezr7f+or4XSXZStvf3ResWybqCdd4b31VcuJbNxZvljQ1vyNcGfk1S2qR4628edbO8s/md6nJuor021wy9xvv/gKwB0rJ5S8kty5WuKV3ltfWvyYNLH5TSfaVeiBj9RribBHd6pfeSc3qcI29tekty0nJq/Nl1w66TKwZeIfOWz5MfnvFDeeTDR7zQNAAAAABwynBvZSckiLRrJzJvXuUb41WT3dmV4ztZtEhk3brKt7KruBAnmzeLvPGGyNe+JpJSOb6Tm28WeefI+M6baK/NNZXjOxkwQKRly8p45CUlIp98IvL1rx8pV1paOent3hCvq6PPxXntNZEHH6zcpwvZUsC3iRsLJs0BAHEf01xLap1U/fdIJCLn9zrfe8M8SDP31bo6aNuybfXfWzRr4b1p7ibjb114q/emee+M3vJx3scxE/bmcaVZ4M++P/b7culfLpXTsk6T9ontZUSnEXWqKwAAAADEtaqY5lrSkfGdRCIi559f+YZ5kDqO76TtkfGdtGhR+aa5O15GRuWb4BZXzr2NXmXv3tqPcfS5uIn+W2+tfNO8d2+Rjz+O/TAAcY3wLACARm1Sn0ny+vrXvcnrKku3LfX+P7HXRC88i3sj3E2uP7bssWM+XvHeYmnVopV0Su7k7fOhpUfFu4t6ckVlfDwXeuXtzW97CUyDfubeZndvn9/00k1y6+m3HnM9AQAAAKDRcfHAX3+9coK5igup4kycWBmexb217Sa7XRiVY+Viq7s314+Oce7edK96I7xPH5F//rPy7889J1JefqSc2664uOZ9u5+1aiXSqVNlfY+OlY64x6Q5AKBR65PRx3vL/OaXbpZhc4bJabNPk1nvz6oO4+LCnox8bKSMnjtauqd2r1NMc8uQ7CHy9UFfl0EPD/KSgep9OocOH/KSfV7wxwvkdxf+rjo0S9DPpoyc4r3N7uoMAAAAAE2Om6R2b5m70Csu5vlpp4nMmnUk9MkVV4iMHCkyerRI9+51i2lucWFaXnqpckLcJQEdNEjkxhtFKioqf/4//1MZh9wdc/lykczMI9u6uOhTphxJBKoNGVIZ9sXt04V60fVFXGsWca/JAQCA46LZ3c2k8PZCSWubVqefOS7sS3Zittx57p0NUFMAAAAAAGAhpjkAACfZ9tLt8qXff0kyEjLk1f989WRXBwAAAACAJo03zQEAAAAAAAAAiCKmOQAAAAAAAAAAUUyaAwAaDZck8+7Fd8uAhwbI4IcHy/A5w+Wm/+8mKdpbdEz7vfFvN8rA2QPl3xf8e63lDkcOy/cWfk96/6639PldH3loac3ZzxetWySjHxstQx8ZKmc8fob8K/df1T9bum2pt84lBHWJS3/97q+Pqf4AAAAA0KgdPChy990iAwaIDB5cmVzzpptEio5trOcl9Rw4UOTfax/ryeHDIt/7nkjv3pXJSB+qYayXn19Zt6o//fpVJhMtKKj8+dKlImecITJiRGUS018z1musiGkOAGg0bnzxRimoKJAlNy6R9IR0cRHGnv30WW9dTck1g+SV5cnTnzwtJdNKpEXzFrWW/dPHf5JPd38qa29dK8X7ir1J7wk5E2RQh0Ex5QorCuWa566Rt77xlveztze97S2v+s4q7+duov+eCffIpf0v9eruPgT4t37/JgPbD6zXOQAAAABAo+Ymt93E85IlIunpIi6a9LPPVq5Lq99YT/LyRJ5+WqSkRKRF7WM9+dOfRD79VGTtWpHi4spJ7wkTRAbFjvUkM1NkxYojy/ffL/KPf4hkZFQuu4n+e+4RufTSyrq7DwH+7d8qJ+7RqPCmOQCgUVhXsE6e+eQZefKrT3oT5k6zZs3kykFXSq/0Xt7yb979jQx6eJAMeWSIN0ldvLfYW3/X4rtk8rOT5ZK/XOK9Ue6SbrrJaveG+oTfT5C9B/fKqMdGyX3v3FdrHRZ8skCmjJziTa67pJ2TB02Wv6z6i6/cF4VfSGZCZvVk+vge42Vz8Wb5aMdH1fWueju+fH+5tG7R2tsfAAAAADQ569aJPPOMyJNPVk6YO82aiVx5pUivyrGe/OY3lRPYQ4aIXHNN5cS2c9ddIpMni1xySeXE9Je+VDlZ7d5Qd5Pee/eKjBolcl/tYz1ZsEBkypTKyXU3Ae72+Rf/WM/niScqJ/yruHpXvR1fXi7SuvWRCXU0KkyaAwAaBTfh3Dezr2S1yzJ//srnr8i8FfPk3W++Kyu/vVISWyXKtNenVf/8n1v/KU999Sn59LufSofEDvLoh496b6cvvGahJLdOlhW3rJBpZ0+TOR/OkelvTjeP4Sa+e6T2qF7OScvx1ml9M/pKfkW+vLflPW/5xTUvSun+UtlYtNFbdhP/d755p3T/n+7S76F+cu+X75WOSR2PuY0AAAAAoNH56CORvn1Fsuyxnrzyisi8eSLvviuycqVIYqLItCNjPfnnP0WeeqryTfEOHUQefbTy7fSFC0WSkyvfDHfl58wRmW6P9WTzZpEeR8Z6kpNTua42770nUlhY+SZ5FTfxf+edIt27V4ZuufdekY6M9RojwrMAAE4Jr69/3XvzuypMy7dHf1uufObK6p9f2OdCyWyX6f19XNdxsnLnSnM/t4y+5Zjrkto2VZ698lm54407pGx/mXc8F3qlZfPKf3bdG+0zvzxTrh5ytawvXC/nPnWujO48mvAsAAAAAKC9/nrlm99VYVq+/e3Kt9CrXHhhZdgUZ9y4yol1yy3HPtbzvWV+3XWVMc2ruDfaZ84UufpqkfXrRc49V2T0aMKzNEK8aQ4AaBRGdhopn+d/Lvl78kOVdyFQjta2Zdvqv7vwKi6paF11T+0um4o3VS+7N8fdOsuEnhPkH9/4hyy7aZn89oLfyvbS7d6k+O49u+X51c97E+aOCy1zRtcz5N3N79a5PgAAAADQ6I0cKfL555VJNsNQYz1pe2Ss54VXcUlF68q9Gb7pyFhPNm6sXFeTsjKRv/5V5JvfPLJu926R55+vnDB3XGgZlxTUvSGPRodJcwBAo9Ano49cPvByLxloVTxwlwj0fz/9X+9t7Ym9JspfP/mrlOwr8X7mwq9c0PuC41qHKwdeKXM/miuHDh/yYqK7GOfu7XbLjtId1X//+Vs/ly/1/JJ3Dult073QMX/f8HfvZ24S3YWOGdxh8HGtKwAAAAA0Cn36iFx+eWVs8Kp44C4R6P/+b+Xb2hMnVk5Qu4Sejgu/csHxHet5b67PnSty6FBlTHQX49y93V4T9/NhwyoTfVZx8dhd6Ji///3IJLoLHTOYsV5jRHgWAECjMe/SefKLt34hYx8f64U6ORw5LOd0P0e+3OvL8pW+X5FVO1fJuCfGSfNmzWVo9lB5+KKH63wMF9PcvRV+z4R7fD+7dui18sG2D6Tvg329N9mnnjFVhmQPqY5b7v48funj3rKLi/725re9N9rHdRsnT1z6RPVb7n+98q/y36/9t/ezA4cOyA/O+IFXBgAAAACaJBez/Be/EBk7tjLcyeHDIuecI/LlL4t85Ssiq1ZVhl5p3lxk6FCRh+s+1vNimm/fLnKPf6wn114r8sEHlbHV3ZvsU6dWJh11Xnyx8s/jlWO96tAsLnHo0dxb7m5y/7//u/Jt9wMHRH7wg8p6o9FpFnGv6QEAAAAAAAAAAMKzAAAAAAAAAABQhUlzAAAAAAAAAACimDQHAAAAAAAAAKC+k+ZvvfWWXHLJJdK5c2cvCdoLL7wQuM3ixYtl5MiR0qZNG+nTp4889dRTdT0sAABAo8dzFIB4Rh8FAABQz0nz8vJyGTZsmMyePTtU+Q0bNsjFF18sEyZMkBUrVsgPfvAD+da3viWvvvpqXQ8NAADQqPEcBSCe0UcBAABUahaJRCJST+7tg+eff14uu+yyGsvcfvvt8vLLL8uqVauq133961+XoqIiWbRokbnNvn37vD9VDh8+LAUFBZKZmekdE0B8ct1JaWmp93ZS8+ZEfwKA2vAcBSCen6PoowDEcx8FACdayxN9gCVLlsjEiRNj1k2aNMl7C6EmM2fOlLvvvvtEVw3ACbJlyxbp2rXrya5GXMnKyvKt0w+bBw4c8JVp1apV4L71fqwBp7WuRYsWMctHD2BrKnPw4MHA4zuHDh2KWbY+nw3zsK3rXd/BtLWdrpMbtAedv1Xvli1bBpbR7WGts/bTunVr3zpdztpOn2+XLl18Za688krfurKyspjl7du3+8ps3LgxZjk/P998UzHo3rbuieTk5Jjl4uJiX5k9e/YEXqNPP/1UThU8RwFNT2N6jqKPApqextRHNZSrr746cBzXrl07X5m0tLTAfQ8fPjxmefTo0b4y1vOwHttYZRISEgKf2d2HoEFjBmsc5UJ2Bdm/f3+t+62JPhdrXKHHQ9ZY1xqP6n1ZddLtFGasF+YaWe0WZhxvjQet4x0vB9WcgHXf/OlPfzKfGY6WmJgY2P7WPMoHH3wgp8ykeW5urmRnZ8esc8slJSVSUVHh+yV17rjjDpk6dWrMoLl79+5e55ySknKiqwygntzvdbdu3XwTXwCA+uE5Cmg6GuNzFH0U0HQ0xj4KAOJ60rw+3Ccr1qdS7iGKBykg/vHVWgA4eXiOAhq3U/05ij4KaNxO9T4KAKqc8EBUHTt2lLy8vJh1btk9EFlvHgAAAKASz1EA4hl9FAAAOFWd8DfNx40bJwsXLoxZ99prr3nrAaCpsOKV6XhdVtwxXaa+sbmteG26nLWdFjamud6XVe8wcfaCtjkWuk5h2j+s+iRHss5Nx/mz4uOFiaGn45A7jzzySGAMdR2b3HFftw86vo4ZGOYecbZu3Rp4TXTMvmPIZ94o8BwFIJ7RRwGAHdNZPyMnJSX5yujY8Fbom/79+8csWx9IWs/s+jnaeq52SZiPtnfvXgkjIyMjsE4uaWzQOFJ/68jK56THHtY4whpr6ONZYwarTfR1a9u2ra+Mrqc1rtHHDxOb3LoGVo4zfTyrjazt9PGsdtNjrYPGddP7scaD5557rm/dmjVrfCGfNH0vhY1zf6LUeVTvKrxixQrvj7Nhwwbv75s3b66OUXfddddVl7/llltk/fr18uMf/1hWr14tDz/8sPz1r3+VH/7wh8fzPAAAAOIez1EA4hl9FAAAQD0nzT/88EMZMWKE98dxSVzc36dPn+4t79ixo/qhyunZs6e8/PLL3hsHw4YNk9/+9rfy+OOPe1nVAQAAmhKeowDEM/ooAACAeoZnOe+882r9KvRTTz1lbrN8+fK6HgoAAOCUwnMUgHhGHwUAANBAMc0BAHZMMR2vWsd0c/TA1YpxrfdtHcuKoab3ZcU0CxNT/XjFC7diwek2sY5vxVkLE1M9TNxx65poYeLMW2XqG5tb78uKYafPzYrFp2MYOpmZmTHLOTk5vjLWPRhUxzBxFq3Yh1ZcxVM9hjkAAAAalzB5l3Rs8qpkykfLysrylUlMTAyM8azHFVaddu/e7Sujxwg6VnlNccZ1DGv9DG+NR6w2Ki4uDtyPNR4IEy9cj4mt8ZDVlnqsY8Xr1sezxidh6tiuXbvAfe/ZsyfwelvXyIqzr/NOWWPdMHMLe0PEvu/QoYNv3ciRI2OW33nnncA6hsl7diLVPVMZAAAAAAAAAACnKCbNAQAAAAAAAACIYtIcAAAAAAAAAIAoJs0BAAAAAAAAAIgiESgANAArOYhO8mElR9FJPaxEHDqBh7Wf+iZU1IlPrOSZYZNzBm1nJfmob+KPMMl49L6t5DBh9m0lh9HrrDbSxw+TmLS+bWslubHaKD8/P2a5ffv2vjI9evSoNTmRk5ubG7OclJQUKjlNamqqBNGJb8K0BwAAAHCiVFRU+NYNHTo0Zrlz586Bz7XJycmBYzZrzGKN9fSztvVcr8cDO3bs8JXJy8vzrdPP39aYTdfJKqOPb42ZrLbVYxudvNPat9VuViJM3W7WGC3MuE0f3xp7h5kjsJJ86jJWu6WkpASeW3l5ua+Mbss2RhvpdVb7W+2tk+GuX78+MBFoQUGBnEy8aQ4AAAAAAAAAQBST5gAAAAAAAAAARDFpDgAAAAAAAABAFDHNAaAB6Hh1Vsw6KzZz2DjXQTHtrLjbOvZZmBjn1nmEicVm0edmxUsLE/fdivOu62nFWQvaxoqpZgkbny6ojHVu1n7CxEuvLx3Xztr3D37wg5jlwsJCX5lnnnkm8P6zrok+3zDtH6YMAAAAcKJY+XsGDBgQGONZ5/OxyuhY4NZYb8+ePb51mZmZMcvbtm0LjFdujT2t2Nj6fK28RHpsZ+Uz0uNfa8xgxd3Wcc6tcYXeztp3UVGRb11QHa19WWOmMONYazt9D1i5qTQrXrwVC75du3aB102f2x7j3tLjb2s8bunWrVvMcq9evXxldu3aFVjHhsSb5gAAAAAAAAAARDFpDgAAAAAAAABAFJPmAAAAAAAAAABEMWkOAAAAAAAAAEAUiUABoAFYSSb1OquMTpgSJummVcZKYGIdT9PJYKxtrH2HSU6pE4a0bt06sI5hEsiErbdOxmLt2zqPMEk+9Xb1TdZpbafrbSVe0clwrHOz2kTva8eOHb4yCxcuDDx+enp6zHJJSUngsWpK9KPpZDhhtgEAAABOlD59+vjW6bGN9Tyskxxa4xqd5NEqYyVL/OKLL2KWExISfGUGDhxY67FqSmCpx0OlpaW+MgUFBYEJJcOMWazj67YNM66xxnCdOnUKTIQZJhGo1W56XZhxvLVOJ1S16mSNh6wEosXFxYH71glkk5OTA69tYmJiqLGeTkTavXt3Xxl9L+n6NDTeNAcAAAAAAAAAIIpJcwAAAAAAAAAAopg0BwAAAAAAAAAgipjmANAArHjd2dnZgbHo8vLyAvetY5jl5+eHiilmxYcLisVmbWPFedMx26wYbrpOVry0tm3b1ho/ztm7d2/g8cNsZ51bmJji1nZh2jYoDntNdJ2sc9PHt+LsWddExxEsLCz0lXnmmWdilkeMGOErk5GRUWtMxZrOV9fburY6ZqEViw8AAABoKFZsbD1Gsp599XOtNWbUz/rWfnbt2uVbp2NBW+NB/exvjRms4+ntrHHcvn37JEhaWlrgs781HtHx0cPU2xqzVlRUBF4Tazyoj2ddN13GGrNZMdx1OR333To36/hWLHA91rLinm/bti1muX379oH71jHOa7on9Lita9euvjKrV68OjIXfkHjTHAAAAAAAAACAKCbNAQAAAAAAAACIYtIcAAAAAAAAAIAoJs0BAAAAAAAAAIgiESgAxAkrgcaOHTtilk8//XRfGZ0cY+PGjaESuOjkJFaSEythTJiEkvp4OqGKlbAkMTHRV0avS0hICFVHnTAlNzc3sIzVRlbCmDB0W1r71mWsdgyTQNVKDqOTyljtbyUr1clZrGQ8JSUldU4ONGzYsFDHt+qp6d+B9PT0wG0AAACAEyUrK8u3Li8vL2a5e/fugeMhKzGkLmM9Q1tjJL2v8vLywLGOdXwryaYej1hjBp1k03rO1/u2zs0a6+nxhzUeCrOfMEk+rX3relr70XW02tGit7OuiU6yau3bam89tmzbtq2vTGpqamAi1pZq3+3atfOVse43fTxr/J+RkRGzXFpaKicTb5oDAAAAAAAAABDFpDkAAAAAAAAAAFFMmgMAAAAAAAAAEMWkOQAAAAAAAAAAUSQCBYAGYCV0vOKKK2KWe/To4Svzq1/9KmZ57dq1gYlArEQkYZJ1WgkddZITKzFmmASiViKSlJSUmOXk5GRfmbS0tFqXa9q3rpO13apVq2KWi4qKQiWMsRLUBG1nbXPo0KE6t791La1rGyapjnXd9PW12rasrCwwyWqXLl1ilvPz8wOTjlp1sur9xRdfBF63sWPH+tYBAAAAJ4KVLLGkpKTWhJ7WM3uYMtazf3FxceB4ZPXq1b4yn3/+eeB4xEoyqs/XOr5O+mglnczMzIxZ7tSpk6+MNR7QCSStNtEJNK1zs5Js6nYLkxzVGiMHjf2s/YQVZjxoJWfV5ax666SeSUlJgfu2xnXWuemxXpg2sa5bQ+JNcwAAAAAAAAAAopg0BwAAAAAAAAAgiklzAAAAAAAAAACOJab57Nmz5Te/+Y0Xx3TYsGHy4IMPypgxY2osP2vWLHnkkUdk8+bNkpWV5cXxnTlzphnTCABORVZMLx2LfNSoUb4y3/zmN2OW//KXvwTG5raOFSZetrWdXmfFdLNiqOl1Ojaatc76NyE9Pb3WOOg1xf4rLy8PjGnev3//mOXly5f7yuzfvz+w3jrGtyVMTHmrba020feNte8w8cvD1NOKM6fvm61btwYezzq+tW8d1946/3fffTdm+aOPPvKV+elPfyrxjOcoAPGMPgoA6sb1f1pqamrMcocOHQLLWPG7dW6gMPGznY0bN8Ysb9iwwVdGxyLX4wxn27ZtgdtZ40hdT2s8mpGREbOck5PjK2PFOXf/1gSNNfXxrRjfYcZD1hhNrystLQ2MaW+Vsdpb79u63npsa+UGs+ix7Z49ewL33bNnz8BY9NaY2Rqj63OxxrF6jGiNGeP6TfMFCxbI1KlTZcaMGd5A1T1ITZo0SXbu3GmWnz9/vkybNs0r/9lnn8kTTzzh7eMnP/nJ8ag/AABAo8FzFIB4Rh8FAABQz0nzBx54QKZMmSI33HCDDBw4UObMmeN9qjNv3jyz/HvvvSdnnXWWXH311d6nRhdccIFcddVVsnTp0roeGgAAoFHjOQpAPKOPAgAAqMekuXvlftmyZTJx4sSYr1i45SVLlpjbnHnmmd42VQ9O69evl4ULF8pFF11U43HcVxTcVxmO/gMAANCY8RwFIJ7RRwEAANQzpvnu3bu9eDLZ2dkx693y6tWrzW3cWwduu7PPPtuLV+Ni2Nxyyy21fmXPxcC7++6761I1AACAuMZzFIB4Rh8FAABwjIlA62Lx4sVy7733ysMPPyxjx46VdevWyW233SY///nP5c477zS3ueOOO7xYelXc2wfdunU70VUFgAbl3sY62tNPP+0roxNfWAk19BtaCQkJoY5vJWzRdDKaMAktrX0nJSX5yuh6WmV0UhMroaR1fN1O1nY6gUn37t19ZVx81vokUNXt1KpVK18ZnTDFurZWUpUwiT816/zDJHANk3jF2vd1110Xs9ymTZtQ56aTilrJkLZv3x6z/OGHH8qpjOcoAPGMPgoA/IkxnfT09MBklToRZJjEkNZztTVm2bFjR0CtRRITEwPHI1YiyDDJSPW4wjo33W56fFzTsXRSz86dOweeW9jxiB5/WIlA9fh7165dvjKFhYW1LteUHFQn57SO37Jly8DxWJixtTUe1EltDxjH79+/f63jasdKBq6vtzXW0+usZKVxO2nuMtS6yYG8vLyY9W65Y8eO5jbuYenaa6+Vb33rW97ykCFDpLy8XG666Sb56U9/ak52uJvZuqEBAAAaK56jAMQz+igAAIB6xjR3nzqNGjVK3njjjZhPAdzyuHHjzG3cpwL6Yanqrbz6vC0HAADQGPEcBSCe0UcBAAAcQ3gW91W666+/XkaPHi1jxoyRWbNmeW8TuAzrVV/J7tKlixerzrnkkku8LOwjRoyo/sqeeyPBrQ8TGgAAAOBUwXMUgHhGHwUAAFDPSfPJkyd78XqmT58uubm5Mnz4cFm0aFF1whgX/+botw1+9rOfefF13P+3bdsm7du39x6ifvnLX9b10ADQaFkDx1WrVsUsV1RU+MqkpKTUGs/Z0W946RhnNcWm1tuFiSlmxUuzzk1/7VrHlLPirOm4f1YsNCumnfUmm44hHma7nJwcXxkrhtvatWsDY8jr+HhW+9d3MkFfgzBv8oV920+Xs6635hLAae+8807gvWXVSd/fVhvpWHhWDMl4xnMUgHhGHwUAdafz8jg6rJUVlkrHObfiXusxkxU/2xrr6OdoPa500tLSYpatkFrWvvXYxhrr6Od/6/z1uVjnZo1t3Ye5RysoKPCV0edinZuVd8oqFzSOscYsqampgTHtrfG/HkeGuSY1hUILaktdRyte/Oeffx4Y033kyJGB8dOtsb0V9zzevqVWr0Sgt956q/enpmQwMQdo2VJmzJjh/QEAAGjqeI4CEM/oowAAAOoY0xwAAAAAAAAAgFMZk+YAAAAAAAAAAEQxaQ4AAAAAAAAAwLHENAcA1I2VUFEn/ti5c6evzOrVq2OWXZKtoCQf+/bt85WxkmyESfJpJXDUrMQnOqlnVlaWr4xeZyUr0cdv3bq1r4yV+ES3gVVHnfjFOlcrOWhZWVnMcl5eXqiENZpOmGMlPbGuyYlMjhIm8acuEyYR6Ne+9jVfmTfffNO3Tt8DOumqlYzGSuADAAAANBRrHDFhwoSY5YyMDF+ZLVu2BD7X6qSPRUVFgWWsZ3brOV+PmaxxpEWPv6xEoDqhpZX0UicitdooTCJMq93CjGOtNtHrrHGkTupp1VGPB61jWddNj/XCJCsNOx7S18RK1qnr1L17d1+ZTZs2xSy7xOFByVqdxMTEOicrDTM+PZF40xwAAAAAAAAAgCgmzQEAAAAAAAAAiGLSHAAAAAAAAACAKCbNAQAAAAAAAACIIhEoADSAgoIC3zqd1GLjxo2+MiUlJYFJVvS6sMkydZKRMIlArf2kpqb61vXo0SNmuWPHjr4ySUlJgQlMdOIV6/wt+tysBC46OaqViMWqU+/evQMT5ujrbSViPXDgQOB+rDrVR9gEKmGSjIZJTqOTuvTr189X5owzzvCt27BhQ8zy22+/HZh45kQmRgUAAACCPPHEE4HjuLy8vMDEjDoxprNu3bqY5fz8/FBJF/W4zRpr6HFcSkqKr8zevXt96/bs2RNYRo/b9Lla+wk7jtWJOK3twrDGiPp4YRKBWmM9PUax2kiPB53i4uLAfSckJATuW5exrm+Y9k5UyTutOn388ce+Mj179vSt02NCaxxZWFgYs7x161Y5mXjTHAAAAAAAAACAKCbNAQAAAAAAAACIYtIcAAAAAAAAAIAoYpoDQAOwYorpeOFWbGYdQ82KO6bj04WNza3jXFvb6TI6fpvTtWtX37ouXbrUGuPainNntVFQjDerHa16W2VatmxZ52tkxWLv06ePr8zKlSsD49Xpa2nFGbSum7WvIPWN+21tF+a++eSTT2KW58yZ4ytz1lln+da9//77gfEZR44cecztAQAAABwvVrzuZcuWBY6Z9DjCyueza9euwDFLmLGGHp9ZY00dh72mMYoeI1ljVJ3TSscvd8rKygKPVVFREXj8Dh06BJ6bFZs8zPjTGuvodVb763GMleOstLTUt06Pm61xtB7HhhnrW/W08oVZxws6vnVuw4YNC7wnrHoPGjQocDzekHjTHAAAAAAAAACAKCbNAQAAAAAAAACIYtIcAAAAAAAAAIAoJs0BAAAAAAAAAIgiESgANIDMzMzABCJWchKdiFEnS7GSfOjEHGETaFhJTnQikJSUFF+Z9u3bByZVsZLj6HU7duwITFayc+dOXxmrTfT5WolQ9LlZCWSs89WJbqzr1rdv35jltWvXBia10QlGazq35s2bBybr1Ovqmwg0DCvxjz6elfhnzZo1vnX6GoT5nbDudwAAAKCh5OXl+dZt2rQpZnn79u2BST5zc3N9ZdLS0gLHTHp8YI3HrPGQ3nenTp18ZXbv3u1b99lnnwWevx5rhRkzWnW0kkzu3bs3Zjk1NdVXRq8Lk1DTGn9YyUn12KawsDCw3YqLi+s1/rbqrc/fGjNZ+9bjb2vf+hq0NsZ6RUVFgWPNtm3b+tbpRLPW8S+55JKY5YsvvlhOJt40BwAAAAAAAAAgiklzAAAAAAAAAACimDQHAAAAAAAAACCKQKAA0ACs+Gw6hppVRsfmtuKF6Rh2hw4d8pVp1aqVb52Oz2bFK9P7ysrK8pVJT08P3K68vNxXRsd+s+qt47PpuHs10XHmrJhuOl66jilo7cc633bt2gVek4EDB/rKfPHFF4Gx8Kx7Qt8DVgw7fd+EaVuLFYte37dW2+o66vjtNbWbVc+ge9k6PgAAANBQrGdY/TxsxW8eOnRozPKgQYN8ZV5//fXAHE9WbqRu3brFLPfs2dNXRo/RNm7cGBg/24oXbsUG37ZtW61x0K28X9b4wBpH6LxPXbp08ZXR8dmtNrLyfuk20XG4rfjk1jXR8enz8/N9ZTp27Ohbl52dHZgbTY+JrbG+NUbS400rzr7eroOR90vHsLfmKKy21WPbMHHPT2RurjB40xwAAAAAAAAAgCgmzQEAAAAAAAAAiGLSHAAAAAAAAACAKCbNAQAAAAAAAACIIhEoADSAMMk5wyR0tBJh6O2sMjrppZWIw9pOJ+dITk72lbESv+h9JSQkBO7bqqM+N+v4ltLS0lqTtVhJZcImq9TXzUq8ohOBWnr06BGYCGXr1q2B1826t/R9E6Y+VuKXMEllrASy+vpbSVata6IT9FiJh/T9FvbcAAAAgBNhz549vnU68ed5553nK9OrV69aEyxa+9mwYYOvjJUsUicC/eCDD3xl9LodO3b4ylgJLDMyMmpNummN/6xEqEVFRYHjA51Q1NpXenq6r8yAAQNilvfv3x94fKsO+/bt85XZtGlTYLJQnVDUGldaY52VK1cGbte5c+eY5d69e/vKWIlPs7KyJIg+t8+MBK6ffvppzHJOTo6vTJi5DStZaEpKSszy7t275WRipAkAAAAAAAAAQBST5gAAAAAAAAAARDFpDgAAAAAAAABAFDHNAaABdO/e3bfOigUdFOfMivMWJn61FXdcx6a26HhxVmy0MDGldRxuq05WTPWysrKY5Y0bN/rKWLHIdQw1Kza3PhcrprjVbvq6Weev29Y6N70fHb/N6dKlS2BcQSsWnD6+FS8+DCsWnV5n3Ue6va1YgFadunbtGti2ep11bwEAAAANxRqj6Wfk7OzswJjaOg62M3DgwJjlv/3tb74yO3fu9K1btmxZYLz03NzcwGd2K157x44dY5bbt2/vK6PHbda56VjsVq4m61lfx4KfNGmSr4zOhVVQUBBqHKnHbdZ4UO/bipeuY7Fb8cutvE+6nazz1+2m49fXdE10Ti1rPKbrOXToUF+Z4cOHB8bZt66lXmcdP0zet4bEm+YAAAAAAAAAAEQxaQ4AAAAAAAAAwLFMms+ePVtycnK8r7KPHTtWli5dWmt595WT7373u9KpUyfvq/79+vWThQsX1ufQAAAAjRrPUQDiGX0UAABAPWKaL1iwQKZOnSpz5szxHqJmzZrlxQ9as2aNdOjQwVfexfY5//zzvZ89++yzXnzWTZs2SVpa2vE6BwAAgEaB5ygA8Yw+CgAAoJ6T5g888IBMmTJFbrjhBm/ZPVC9/PLLMm/ePJk2bZqvvFvvAu6/99571QHd3ZsLANCUWIlXNCvJiE6EaSV91KwyVpINnVTESjKik2NadbSOp7ezEoHo5Dg6waWzevXqmOWSkhIJo7S0NGY5ISHBV8a9EReULCVM4herjD5fq410oh2rjvpYVqIXq0308a3EO9a63bt3B9ZbJzC1kqyGubd0chxHT8i0a9dOglhJVuMZz1EA4hl9FADU3datWwOf2VNTUwMTeFpJN/UYqXv37oHHcgoLC2t9zncyMjICE2NaH5jqsZ41HtDJKq2xnj5f69lf19G55JJLAsdxejxoJeK0xlHNmzcPHGt07do1cMyk20SPoWrat058abWJHjemp6f7yqxfvz5wbK0T0TqXX355zPL48eN9ZRITE2OW33///VDJcXU7Weevx8h6ziCuw7O4NwlcBt6JEyce2UHz5t7ykiVLzG1efPFFGTdunPeVPZctePDgwXLvvfeaEyhHN5K7eY/+AwAA0JjxHAUgntFHAQAA1HPS3H0y5R6A3APR0dxybm6uuY37dMN9Vc9t52Lb3XnnnfLb3/5WfvGLX9R4nJkzZ3qfwlX9sT41AwAAaEx4jgIQz+ijAAAAjjERaF241+/d1zkee+wxGTVqlEyePFl++tOfel/1q8kdd9zhvcpf9WfLli0nupoAAABxh+coAPGMPgoAAJyq6hTTPCsry4uvo2PzuuWOHTua27iYsS6Wz9FxeU477TTvbQX3FUArro/Luu7+AAAAnCp4jgIQz+ijAAAA6jlp7h563BsEb7zxhlx22WXVbxe45VtvvdXc5qyzzpL58+d75aoC6q9du9Z7wLIeogDgVGQlR9GJMKw+USf+CJOI00q6GIZOOmIlGbHK6GQpTlJSUmAZnYzFKpOWllZrYhBn1apVvnU6lmrnzp0DE8/oRDyO/oq6lfhGL1uJZ6zkMPp8reRAbsJB00l8rKQ2+rrpZDU1baeTg1rJiHSdrPs2zL1ttVuYxLf6HrTum3jFcxSAeEYfBQD1k5KS4ls3YsSIWp+zLQcOHAgsYz2fW8/VehxjJfnUz9HWfqxkjWHGDPrDUZf/Img8YuW4cLkyND22s8aI+tysMYM1ttXtFGaso5OeOqeffnrM8o4dO3xlrPBkYRKI6vG+NddgjWM3bNgQs2x9IK7v22bG3IIe61rJWq1EoLqe1r7170CY34kTqc4jzalTp8rcuXPl97//vXz22Wfy7W9/2/uFqcqwft1113lfuavifu4yqt92223eA5TLvu6Sw7hkMQAAAE0Jz1EA4hl9FAAAQD3eNHdcnLpdu3bJ9OnTva/dDR8+XBYtWlT9KcbmzZtjPsFxn5y8+uqr8sMf/lCGDh0qXbp08R6qbr/99roeGgAAoFHjOQpAPKOPAgAAqOekueO+nlfTV/QWL17sW+e+hvH+++/X51AAAACnFJ6jAMQz+igAAIB6TpoDAOomPT09MIaajkNtxebesmVLYEw1K6aZJUwsaB2LzoqpZtVbx8OzjqXj41mx8HRcOyvGdZ8+fXzr2rdvHxgbXMe+021d03XTcdat7fQ66/z1+erEa862bdt86/S56HN1CgsLA2MRuoRvQTH0dbw6K/bcwYMHfWX08aw4j2eccUa9Yvjr41vnBgAAADQUnc/JGg9Zz8w6NrgVd1xv574NpH3++ee+dR06dAgcs+hnbauMNY4Js2891rLGejqmud5vVXLpoPGnVUc9ZrDGNVZ767G0NY7U19Yaj4U5lrVvXW9rPKTXWWWsOO96jOi+IabpXFj7jTrq+926tla8+DDzFPpessb/DanxZM8CAAAAAAAAAOAEY9IcAAAAAAAAAIAoJs0BAAAAAAAAAIhi0hwAAAAAAAAAgCgSgQJAA0hNTQ1M8qETilgJFK3kGXo/YROP6KSLVrIOXUYnBq0pOahmJWfRx7POPzMzM2Z569atoc5Nt4mVQEQnmrES+Oikn05iYmJgcpQwiVh0Ha3jDxw40LeuuLg4MPFMUVFRYLJM65ropDpWUp8wiXcOHDgQs5ydne0r079//8A2CXNvkwgUAAAAJ5M1HtBjHZ300hoj7t6921dm27ZtgeOxjIwM37ouXbrUOq606qgTk9Y0/tRjBOvc9BjRGlfofQ8YMMBXpnPnzr51etxojWP1eMg6Dz2us8aW1nhEn4vVtnqMpMfVNbWbHrdaCWT1uNGqY0lJSeD59uvXT4I0C3H+VtuGGaNZ102PNcvKyuRk4k1zAAAAAAAAAACimDQHAAAAAAAAACCKSXMAAAAAAAAAAKKIaQ4ADUDH5g4bC8yKfRYUL8yKDRYm7rhVRsdLa9myZah4ZToWm3Vuep0VG1vHULNi+OXn59crzp0+fnp6eqg20fuy9q3rabWbjqlutaO1Tt9LVnxAHVfPOg+rvXWcuzDX1jp/vW8dU9E6Vk0x+4LuCWKaAwAA4GSynmvD5ArSMa2t2OSFhYWBz/XWs3ZQXiRrzKLrHJY11tuzZ0/geEiPvzp16uQrY9VJjzWs/FW6va2xjzX20GMba/ypr5uVv0qfr3WP1Lfd9PGs+OXLly/3rdPlrH3rdjpk3LdhxvrWdQszjtPzH1bes4bEm+YAAAAAAAAAAEQxaQ4AAAAAAAAAQBST5gAAAAAAAAAARDFpDgAAAAAAAABAFIlAAaABjBgxIjA5RkVFha+MTnzxz3/+01dm//79gQk9wiTwsJLK6Drq5B01JYLUdbASgejEHwcOHAjct3VuVnIQfb7WdsnJyYF1tM5N19tKPKPbKUxSHV2fmpLT6MQzVrJY3SbW+VsJW1JTUwMT5tQngeyAAQN8Zax7SZ+LVUbX2yoDAAAANJT+/fsHPrNaYx3NSiiZkpJS67KTnZ3tW9erV6/AJKPbtm2rNQllTeMhPf4sLy8PNdbQdOJPPRapaaynx01W2+pxjJV0Msy42Sqjk4Na+9ZlrDpa4zg9ttJtba2zymRlZdUryak+/l419rTOpaysLNT8Q5hxm97OGms2JN40BwAAAAAAAAAgiklzAAAAAAAAAACimDQHAAAAAAAAACCKSXMAAAAAAAAAAKJIBAoADSA/Pz8wqYmVdFEnULESYVgJWzQrOYlOxGEledHHs5KFWPXW66yElvp4Vh31fqykm1ZyGL0vKxGNLmPVMUwCE6veVlsGsa5tenq6b51O/mO1v07OYiVwsdpSJyy1zk3X02qj4cOHxyx37NjRV8ZKRqST+lj3m75O1nUDAAAAGop+hrWex62EkjqBpvVcq5+HrWSK1nb6+b9bt26B44qtW7f6ypSWlvrW6TpY44EwCR11m1hjKD0+sc6tuLjYV0Ynp7TGg1ad9LqKigpfGV3PMOORxMRE3zorgaduW+t663Ox2sgaI3bu3Dlm+bTTTgs8t1ZGu+l6W+dv3e9hxsj6vgmTUPZE4k1zAAAAAAAAAACimDQHAAAAAAAAACCKSXMAAAAAAAAAAKKIaQ4ADUDHtAsb503H/WrXrp2vTElJSWDcLyt+tY5XbcVL07HIrNhoVgwzHefMOn99bla8uIKCglrr7Ozbty/w+JauXbsGnkdSUlK9YtjpmHVWTHFdb6uNrPPV52Zdb70uzDWy6m3dk7pOOTk5vjJnnXVWzHL79u19Zax7Scd1tO4JfS9b9y0AAADQUKyxls47FSZXjzUe0GMGKy60dXw9/rDyQOlY2NnZ2b4y1lijqKjIty6oTla99dgjKyvLVyYvL8+3bvv27YHtptdZ42jr3HROKWvfeoxixRTX45ow46qw8dH1GN3axqr3iBEjao1pb8Wwb2FcN91uYfZjzX9Ycc91Gevebki8aQ4AAAAAAAAAQBST5gAAAAAAAAAARDFpDgAAAAAAAABAFJPmAAAAAAAAAABEkQgUABqAlYgjTAJDndTDSiCjE2hYSV6s4+vkMDrphlVHKxFHmCSTYRKaWMlCdHIUnVDHKSsrC9yXlawyMzOz1mM5ycnJvnX6GlgJNXV77969OzA5jNW2OjmOVc66bvVJjGrV29p3WlpazPKFF17oK9O9e/fA628lR9X3m5UcRiejCXP+AAAAwIliPcfrpPc6oaf1jGw9M+tEmNbzsTWutJJcBtXbGmuGSSBq1VsnC+3WrZuvzNChQ2OW161bF7gfqy2t89dlrPGwNdbT400rEWaYRJy6jHX9rfGn3s4qo8dRVhtZ9dZjNKtN9H1z2Li3wyQCtcbtYcax+nytMWND4k1zAAAAAAAAAACimDQHAAAAAAAAACCKSXMAAAAAAAAAAKKIaQ4ADaCwsDAwFpiODWfF+WrXrl1gvDIr7pwV01rHGbPilYWJH22t08ez9q1jqFlx3kpKSgLjl1vn1qlTp8DY4LqdWrduHSoWoI5PaMWw02WseHm6ba14bTqGodO5c+fA+yZMTPnU1FTfOusaaAMGDIhZHjVqVGAsOuv41rF0m1gx/PT9Zt1bAAAAQEOxnnV1fHBrzKTHH9bzuX7WDROr3NrOioUe5rlaj2usZ3QrNrZ+rh88eLCvzI4dOwLzQFljJN221rhCn5t1jXSOKeuaWPHS9TorprhuI2tcY7W33rd13+hxvNVGSUlJgeusaxtmrHtAXW+rjNVu+p4IM/YMm5srrt40nz17tpdUzU04jB07VpYuXRpqu6efftpruMsuu6w+hwUAAGj0eI4CEM/oowAAAOoxab5gwQKZOnWqzJgxQz766CMZNmyYTJo0SXbu3Fnrdhs3bpQf/ehHMn78+GOpLwAAQKPFcxSAeEYfBQAAUM9J8wceeECmTJkiN9xwgwwcOFDmzJnjhQuYN29ejdu4rxNcc801cvfdd0uvXr3qekgAAIBTAs9RAOIZfRQAAEA9Js1dLJlly5bJxIkTY+LduOUlS5bUuN0999wjHTp0kBtvvDHUcVxcGxfH9ug/AAAAjRnPUQDiGX0UAABAPROBuqD87k2C7OzsmPVuefXq1eY277zzjjzxxBOyYsWK0MeZOXOm96YCAJwqdLIOK8mGlcBDJ9Cwkj5aSTaCjmWtsxKRhEmyaCXn0OusxCe6Taw20ok/rUQobqCupaSkBCbMWb9+fa3HctLT033rdFJPq/31vqwkJ7r9rWtkJaexkspqOtGPte8wiWcyMjJ8ZawERUGsRKxWe+ukMtb11udiJb6NVzxHAYhn9FEAUD/WWEePh6znWp0Y0Ure2L59+5hlK1xWmCSXYRJKhhkzWuPWiooKX5nMzMzA8dAXX3wRs/zCCy/4ymzdutW37rrrrgtMcqrHCFa7bd++3beuZ8+eMcvdunXzldFtabVRQUFB4HjIqre+lla76XsrTEJNaxxnjQf18SPGva3HbNb1D7NvazudiLVRJgINq7S0VK699lqZO3euZGVlhd7ujjvukOLi4uo/W7ZsOZHVBAAAiDs8RwGIZ/RRAADgVFanN83dw5D7JCwvLy9mvVvu2LGj+amRSwpzySWX+D5tcJ+orFmzRnr37u3brk2bNt4fAACAUwXPUQDiGX0UAABAPd80d6/Jjxo1St54442YByO3PG7cOF/5AQMGyMqVK72v61X9ufTSS2XChAne362vOQAAAJyKeI4CEM/oowAAAOr5prkzdepUuf7662X06NEyZswYmTVrlhcryWVYr4ot1KVLFy9WnYshNHjw4Jjt09LSvP/r9QBwKrNigYWJV6bj3OkYX1a8MGs/YeKlWzHF3Feva4sVfizx0sPEQtPxu3v06OEr069fP986HYvbig2uz82KBbht2zbfOn0NwsT5s+J367fsrJh2+vo7RUVFgdvpeIhWHa321vHhhwwZ4ivTrl07CaLrbd3/Vr11zDrrvtH3t76P4h3PUQDiGX0UANSd9Vyrn1Gt52H9rGvFHa/qV6vs2LEj1FhPP1db4wE9/rHiR9c3D5EuY8Umd99WOpr+plNN4z/NGsfpeO36WM5zzz0XOB7S8dOteO3W9ddjJuv6W+MxHYs9TLx86xq5D7aDYppbx9dzBHuMvGOadfww94R1bmHu7bieNJ88ebLs2rVLpk+fLrm5uTJ8+HBZtGhRdcKYzZs3m7+MAAAATR3PUQDiGX0UAABAPSfNnVtvvdX7Y1m8eHGt2z711FP1OSQAAMApgecoAPGMPgoAAKCOMc0BAAAAAAAAADiVMWkOAAAAAAAAAMCxhGcBABw7nVDSSmASlDzSStZhJU+09q1jklpJNnTSyY4dO/rKWAk89L6tpDI6qYhV71atWgUey0o8opOx6IQqViIUK4GLlWhHt4mViFLvS5+Hdf31dayJTthiJX7Vx09ISPCVsY6nk9pY56aTuoSpt7WfMAlFwySQse4tAAAAoKGUlJQEjoes8Yh+ZrfGNUlJSYFjJiuho96XS+oc9KxtPVfn5+f71uk6hEmEun379sAx6pAhQ3xlrPHn7t27A8dauk7p6em+Mueff35gW65bty7w/K3xkG5bfR2dFi1a+NbpMXl9x9HnnHNO4D0ZJjntfuP4ervi4mJfGet8dTtZ5x9mHNmQeNMcAAAAAAAAAIAoJs0BAAAAAAAAAIhi0hwAAAAAAAAAgCgmzQEAAAAAAAAAiCIRKAA0ACvJhk5qYSVr1Ik/rUQcOvFJmKSfVnISK4HIF198EbPcpUsXXxmr3jqBiZXAQycM0Yk5reN9/PHHgXW0Ep9Y7V9YWBhYxkpqk5iYWOcErmGSZVoJhKxEmPo6WQlc9TWx9qPPw0oQlJaW5iujE7ZYCVx0W4ZJDmTd79Y10cK0PwAAAHCiWM/xOhGiNWbSSSetRKA66aPFeta3En8GjSus41vjT/1s36tXL1+Z5OTkmOXly5f7yvzrX/+KWe7evXuoc9u3b1+tx7LO30pMmZOT41u3Y8eOWo9llenatWu9ErhaY3S9zhrr6XG0NUa3zk3fp2HGUS2MsZ6+J6w2yszMDNy3NdbT52+NWRsSb5oDAAAAAAAAABDFpDkAAAAAAAAAAFFMmgMAAAAAAAAAEEVMcwBoAFa8sjDxm3UMNyumnBVnLCjuuRXDzIq7reN+b9iwwVfGij0XRkpKSsxyu3btfGV0fDYrNnZeXp5vnY6rZ8VU07HgrP3k5+eHilkYFK9bxyt0li1bFrPcvn37UG2r7yUrPp6+3ta9ZcWH0/db27ZtA+MqWsfXxwsTry/svRwmXjsAAADQUKxcPXrcYj0z62ffrKyswGd2a3zSqVOnwLGmNR7UrGf/jIwM3zodw9vKg9S6detal52ioqLA+N3W+M+KYR7Eyp9lxXDX52uNT/S+rH3rsY4V99saD+mxVpgyo0aN8pWxxvbWvjR9nzYz9qPHw/o61hRnXd+T1u+Nbifr96Yh8aY5AAAAAAAAAABRTJoDAAAAAAAAABDFpDkAAAAAAAAAAFFMmgMAAAAAAAAAEEUiUABoAFYCEZ1UIykpKTBZpZVkI0ySESuBShg66aKVUDI3N9e3TieDsRKI6AQuVgIVnbBGJ/isKanM8uXLY5Y7d+7sK9O/f/+Y5Z49e/rKlJeXByZesZJ8rlu3rtb6WEl9rHOzkurohDVWAlWdMCU9Pd1XxtpOJ2PRyWKtfVn3tt5PfZN+WveNvpfre28DAAAAx4OVQFOPY6znYT2OsZ5rc3JyApN+FhYWBo7jrDGTTjxqjZmsMZret/XMrsexHTp08JUZNmxYzPLGjRt9ZaxEkLqdDh06JEGsMbI+D2u8a41Z9DprPKjLhBn7WMe3xqN6rGWNGS1W4k1t//79gePBZDWOt5LFWoln9b6tezLMeLQh8aY5AAAAAAAAAABRTJoDAAAAAAAAABDFpDkAAAAAAAAAAFEEAgWABmDF9NIx1Kx4XTp+dl5enq+Mjo9mxcuzYsG1adMmMBZbmHh9Vmy00tLSwHhxOhaa1UZh4lV37drVt27v3r21xk9zPvzww5jl9u3bh2o3Xc8dO3YExjS32nbIkCGBseCseuvrbcUQ1Puy2siKIaf3ZcX+07HYreum4wpadbTWWe0dZjsAAADgZNHPx9Y4JswzrDWu0s/1o0aN8pV59dVXfev0+CszMzMwprk1ZrPoMalVbz0mtXJj6VjcYfI5Wazjh4lzHmasadU7TNxv3ZZW2+qxfticZkHj+rB5zvSY3RpHHzbGZ/peto4fZowYJs679bvVkHjTHAAAAAAAAACAKCbNAQAAAAAAAACIYtIcAAAAAAAAAIAoJs0BAAAAAAAAAIgiESgANAArgYVOGGIlZtRJJq1EJDqpyK5du3xlrOSgOjmHlQhFJwfZvXu3r0yHDh0CE5hYUlJSApPj6KQuVgIVazudwKS8vDwwEcm2bdskDJ0MxUqyopOcpqam+sq0a9cu8FhWAhu9Xb9+/XxlOnXqFHgsK6mMbl/djlaCHiuBS32TdertrPsWAAAAiCfWGE0/x1rJIvVzdJhn6D59+vjWvfLKK75127dvDxyzVVRUBI49rISOut7WWCvMmEmPNayxnlUnvW+rjL4m1jWy2lsntbTK6ONbyTJ121plrASmOjmolSxUt3e3bt18ZcIkHg2TiLO5MR7T+7HGjFa7WfeSpq9lmG1OJEajAAAAAAAAAABEMWkOAAAAAAAAAEAUk+YAAAAAAAAAAEQxaQ4AAAAAAAAAQBSJQAGgAViJX3RSDSvJpk7qaSXCKCsrC0xyYiX51MlIrESkOhFHXl5eYLIUq55WYlCdHMRKRNK2bdvARCRW2+rjW22ik7NYiTGt4+nzDXNtreum29+qo7Wuf//+tSb9tBJ/6na0ksw6mZmZgQlswyQwrW8CT72dlUDGahMAAADgZAnz7GslgtRjtDBJJ9PS0nxl0tPTfeu2bt0as1xQUBC4bz0WqGmMqNdZz+d6/JGSkhLYbvv37/eVyc3N9a3TY9IwiViTkpIC62idm1UnzRrr6bGmlSzVSqCpx/Y7d+70lbnssstilnv27Bm4H+vcrOSs9RlrtTHmA6z5hzCJbvU9Wd9x5fHCm+YAAAAAAAAAAEQxaQ4AAAAAAAAAwLFMms+ePVtycnK8rzKMHTtWli5dWmPZuXPnyvjx472vi7g/EydOrLU8AADAqYznKADxjD4KAACgHjHNFyxYIFOnTpU5c+Z4D1GzZs2SSZMmyZo1a6RDhw6+8osXL5arrrpKzjzzTO/B61e/+pVccMEF8sknn0iXLl2O13kAQFxbuXJlYJyz7du3B8YC69atm69M+/bt6xyvzoq9ZsUUP3DgQGDcMyvOeY8ePWo9V2s7fSyrjla8NCvOmd7Oiv0XJn52aWmpb51uA6vddNxvK86dvrZWO65duzYwZqF1TXTMQCsWnxWbXMdnt2IPholzZ7VJGGGuiY6zHyY2XjzhOQpAPKOPAoC6s+I36+dq6/lYl7Ges/U4wooxPnLkSN+6d955J2a5c+fOgTG1dT6tmuKl638PkpOTfWX0uVhj1Pz8/JjlwsJCXxlrjKjbMiEhIXBcYcUPt9bpfVk5nvT4wxpr63pb94g1RtuwYUPMsvs3WLv88ssDj28dT18TKzeXPrcDRvvr8ViYsa7FOn+9b+vaxvWb5g888IBMmTJFbrjhBhk4cKD3QOUG3vPmzTPL//nPf5bvfOc7Mnz4cBkwYIA8/vjj3gV94403ajyGmwAoKSmJ+QMAANDY8RwFIJ7RRwEAANRj0tx9CrBs2TLva3dHf3rjlpcsWRJqH3v27PE+qcjIyKixzMyZMyU1NbX6j/VmJQAAQGPCcxSAeEYfBQAAUM9J8927d3uvymdnZ8esd8u5ubmh9nH77bd7Xws5+mFMu+OOO6S4uLj6z5YtW+pSTQAAgLjDcxSAeEYfBQAAcAwxzY/FfffdJ08//bQX+87FvKuJi1lrxa0FAABoqniOAhDP6KMAAECTnTTPysryAu7rZGVuuWPHjrVue//993sPUq+//roMHTq0frUFgEbKSvKpEziWl5f7yuhBZVFRka/Mzp07a91vTYk4dJINK8mJ+5p1UNJNK1mmfmusU6dOgYlI3Ntm2rp16wKThVj//ugkl1biHX0u1vm7f/c03b66Ha111vE3bdoUs7x+/XpfmczMzMCkPuPHjw88/61bt/rKWAlbdNK2fv36Bd5LVsIinUDGStZprdPtFCbJp3VPxiueowDEM/ooAKgfK1miTrJoPXvr52gryad+rrfGdRdddFHg8V966SVfGRciq7blmsY6OoFnRUWFr4xeZ41R9XO89exvtZsuZ5XR4+gw18hKDmq1t75O1gfFaWlpgef/2Wef+dbpf28vueSSwDparGsSJsmmHv8fMq6/bhOr/a3kpEH7se4J6xo1pDqNNN2NMWrUqJjELlWJXsaNG1fjdr/+9a/l5z//uSxatEhGjx59bDUGAABohHiOAhDP6KMAAACOITzL1KlT5frrr/ceiMaMGSOzZs3y3o50Gdad6667zntTzSV4cX71q1/J9OnTZf78+ZKTk1MdDy8pKcn7AwAA0FTwHAUgntFHAQAA1HPSfPLkybJr1y7v4cg9FA0fPtx7q6AqYczmzZtjXqd/5JFHvK/TX3HFFTH7mTFjhtx11111PTwAAECjxXMUgHhGHwUAAFCpWcQKRhpnSkpKvNhKLt5tSkrKya4OgBrwu1qzYcOG+dbpuGY6fphTn0RZVvwyK+6zjh9txRSz4qxrycnJvnU69rhVJiMjI/BcdQw1K85d165dfeuqBve1xWvTdbTipVnrdDw8K86b/qdVxy93li1bFrNsvZGn28iqk1Xm3HPPjVkuLCz0lbGOp2PvWffEkCFDao2zaNUxbExzHR/QekTR97LV/qeffrpvXVNG3ww0Dk31d7WpnjfQ2PC7WrOlS5cGjrWs8Yh+1raea/V+rLjn1jodn/ztt9/2lXn44YcDczxZY0v34WpQvO4wuYr0+VvHt7bT4zhrzKDP38oxZcUG1+vcfa/pcbt1fH0u1thL5yZz7rnnnphlK3+VZtXROjc9RrNyk+mxbomxb31NrDkD634P02/o+80aj1544YXSUBpP9iwAAAAAAAAAAE4wJs0BAAAAAAAAAIhi0hwAAAAAAAAAgCgmzQEAAAAAAAAAiPJHVAcAHHeHDx/2rSsrKwsso5OMWAlE2rZtG3h8K6GlTjRjHV8fz0oEYiX+0MloioqKfGV0Ug8rMUirVq1qrbPzxRdfBCanSU9P95XRiUetJDf6+FY7WQlc9fHz8/OPWyJS3U46WYuzatWqmOWePXuGSs6ir9u2bdsCk6z26tUrsN2se8tKTqs1glzlAAAAaOKspItZWVm1PvtbCTStsYc1/tOshI5636effrqvzDXXXBOzfP/99/vKdOjQwbeuffv2geNR/Ryvt7HWWck6LXr8Y7WRHn9Y4wrreHpMqMfsVpktW7b4yujtrHFV7969fesGDBhQ5/vGGjNa56vLWeMxfS/vM5K86vvUGmtb4z+dQNQax+rtwowZTyTeNAcAAAAAAAAAIIpJcwAAAAAAAAAAopg0BwAAAAAAAAAgipjmANAArFjgOoaaFa9bx922YprpONQ6VlhNscB0nDPr+GH2o4/vdOrUKTCG2+7duwPrrePTWXW0YrjpfRcUFATGYrPOw7puxcXFgfXW6/R1tGKKWzHdrPiEOqa5FVP+s88+i1keMmRIqBh+ug6JiYmB8dqtOIft2rULjKlntZsuZ91vOhb+yY5zBwAAgKbNyhWkn9GtcUxmZmatz9BW/iQr7rl+PrbWWeOK8847LzA2+4oVK3zr9PO/Fb9at4lVRrdJ2HxGehwRJn+XJUyc8z59+gRet/fff99XZu3atTHLl19+ua/MhP+/vXuPjaL6Ajh+saWt1UKp5W0BAXmIKIFCLY8gSlJSIvKPEDCIBkEDJgYSFAVTFRSCSFCoElSsfxirGCBGCIooMbxCeCVVHkYqVFSqRV5ioTzuL3d+W9LePWVmi8zMtt9PUnGHO7tnZ3bvnjtszxk2LGqbvbaRzomXuuNe6oVLx8h+/qnCa9I+/tIYKW77eEuvWy911/3EShMAAAAAAAAAgAgumgMAAAAAAAAAEMFFcwAAAAAAAAAAIrhoDgAAAAAAAABABI1AAcAHUrNGu4mL1AjDbs4hNTmxm1xKzWGkBhr2fUuPbzcCkR5faqBSXl5+zfuRHl9q1mk3K0lLS/PUeMdLjPY2qennsWPHXPezG3NKvDRZ8XKOpG1Ss077uJWVlUWN6dixY9Q2+/xKTWXsMSdPnnRt4Orl+NfVHNTt+XvZBwAAAPCT3XhTaoTZsmXLWrdLSkqixthrO7sJZV3roaqqKtec2V7/9O/fP2pMRUVF1Lbff//9mjFKcR49etR1XSGtR700tJTWqF7WHklJSa5rjcrKyqgx//zzT63bu3fvdl3r3XfffVFjMjMzXZ+vtB6zz5t0jLysP+3XiDRGe1jHSTFK638vx99ufGvf9hvfNAcAAAAAAAAAIIKL5gAAAAAAAAAARHDRHAAAAAAAAACACC6aAwAAAAAAAAAQQSNQAPBB8+bNXRuIeGmWKTWLtLdJTWak5jBe9rMbb3hpVmqcP3/e9fnbTT2lJjMnTpxwbfKSnJxcr+agdsMUu6FNXewml9I5sc+t1BzHfnzp2HppspqRkRE1xr6vw4cPR43p0KFD1Db7+EqvSXub1MD19ttvd70fL81pJF4aBgEAAAB+OX36dNQ2e20jrYdsUkNFe10j5dB201GJtNaz82ipMWTXrl2jtu3Zs6fW7VtuuSVqTFZWVq3bp06dihpjP5fU1FRP64Nz585dc30mrdHsdWVdTSbtmKQGqvZ5khpxjhs3znU9aDf0lJpjemnyKZ03ad1sry2l+7bX1heE16R9TKTXVkpKiuu5lPazj5MUo5/4pjkAAAAAAAAAABFcNAcAAAAAAAAAIIKL5gAAAAAAAAAARFAIFAB8INX0smux2belmmYSu16bVL9M2ualpphdL02qsS2x69pJ+9n1yaRaeDapFp5dG12KW6r9Z9filsZINezscyKNsWvBSefRrvsuPb5Ur9yuly7V4rNrH0rnVqp9aJ8n6fErKytd68zZz02qTyjF5GWM9FoGAAAAgmLnvkarVq1c1zr2+k+qH26v9f7666+oMVK97Hbt2rnm7Paawc7z61pr9O/f/5o9l6Q673fccYdyU1ZWFrVNism+L+nY2mum9PT0qDGlpaVR2+za59Lzb9OmTa3bI0aMiBrTtm3bWrc7deoUNebkyZOux01aR912222ux0hao9rnyT5GXl2x1mjSa0t6fPs8eamp7qXn1Y3EN80BAAAAAAAAAIjgojkAAAAAAAAAABFcNAcAAAAAAAAAIIKL5gAAAAAAAAAARNAIFAB84KWBptQsw26yYTdrkRrISI1gLly44NpQUWqg0rp1a9cmL9LjSU0t3Y5JVVVV1JgWLVq4NtmRGpjYzVCkY2vf16233uraeEeKU3qu9hjpGDVv3ty1Eaz0fO3XgHTfiYmJrg01pcdr1qyZa3MWO27pvNnHX2pgIx1bO06p6aeXxjMAAACAX6R89Pjx465rDbtZ5LFjx6LG2I0QpXWN1ED07Nmzrvdt5+N33XWXa0NTqRHnjh07osb07NnTtemm3WRTWld17949apt9DHbt2hU1xl5H9OjRI2pMy5Yto7YdOnTomvdjjBo1qtbttLS0qDH2c7HXZ3Xdt73+y8zMdF3/Hjx4MGqMtP6ym6FK1yjs5qTJVmNO6fhLz62+63/7vqTXu5/4pjkAAAAAAAAAABFcNAcAAAAAAAAAIIKL5gAAAAAAAAAARFDTHAB8INVvtnmpBSbVy7NrXEv1y6VaYHZ9PKmm2YkTJ665T10xZWRk1Lp95swZ1+cr1ea279uuw2b8/fffrvXhpHpp9n1Jx0iKya4PKJ03+3zbNb6luobSY0nnxN5m1xiXauFJNe2SkpJc45Zet/Z9SzHadfak8+blPSGNsY+3l34BAAAAwI0i1aa2STmrvW6Teg7Z+bBUG11aR/z222+uaxa77ra99qtrrWHH0L9//6gxdi8su1a2195cUk1tu1681L/J7XjUVS/81KlTtW737ds3akxubq5rvXZ7/Sn1qpLW0fa68ddff3Vda0nrf+m+7TWhtEa1X28Xhfuxz5NUi95LTy3pPWHvZ/fc8hvfNAcAAAAAAAAA4HoumhcWFqpOnTqplJQUlZOTo3bu3HnN8atWrXI61ZrxvXv3VuvXr6/PwwIAAMQ98igAYcYcBQAAUI+L5p9++qmaMWOGKigoUHv27FH33nuvysvLU3/++ac4ftu2bWrcuHFq0qRJau/evWr06NHOzw8//PBfxA8AABA3yKMAhBlzFAAAQD1rmi9evFhNnjxZPfHEE87t5cuXq3Xr1qmVK1eqWbNmRY1/66231IgRI9TMmTOd23PnzlUbN25Uy5Ytc/aVmHo8NWvynD59us6auADCo/o9So1hAJCRRwEIcx7FHAUgzHMUAIT2orkpZL979271wgsv1CoSP3z4cLV9+3ZxH7PdfFuhJvNthbVr19b5OPPnz1evvPJK1PasrKxYwgUQENNARGoY0pjxjSsA5FEAwpxHMUcB8IK1XrRHHnnkP7mfwYMHq/9K165d1Y1iSnjV9OCDD6qGYt68eTHv06VLl3o9Vq9eveq1H0J60byiosLpitq6deta283tgwcPivuYrrbSeLvbbU0mUauZfJnutR07dlRlZWVxNzmbf401CaDpeBt019dYEXsw4jl2802hDh06qIyMjKBDAYDQIY9qXJ+JxB6MeI496DyKOapxvd6IPRjxHHvQcxQAhL48ix+Sk5OdH5tJouLtg6WaiZvY/UfswTDfSgIABIM8KlyIPRjxHHtDz6OYo8KF2IMRz7E39DkKAKrFNNtlZmaqhIQEVV5eXmu7ud2mTRtxH7M9lvEAAAANEXkUgDBjjgIAAKjnRfOkpCTVr18/tWnTpqvbrly54tzOzc0V9zHba443THOYusYDAAA0RORRAMKMOQoAAOA6yrOY+nMTJ05U2dnZasCAAWrJkiXq3LlzVzusP/bYY6p9+/ZOgxfj2WefVUOHDlVvvvmmGjlypCouLla7du1SK1as8PyY5tf3CgoKxF/jCztiDwaxByOeYwcAP5BHxYbYg0HsjTd25qjYEHswiD0Y8Rw7ANRHE621jnWnZcuWqTfeeMNp8NKnTx/19ttvq5ycHOfv7r//fqeTblFR0dXxq1atUnPmzFFHjhxRd955p1q4cKHKz8+vV8AAAADxjDwKQJgxRwEAANTzojkAAAAAAAAAAA0RbY8BAAAAAAAAAIjgojkAAAAAAAAAABFcNAcAAAAAAAAAIIKL5gAAAAAAAAAAhO2ieWFhodOJPSUlxenOvnPnzmuON13ae/To4Yzv3bu3Wr9+vYqH2N977z01ZMgQ1aJFC+dn+PDhrs81TMe9WnFxsWrSpIkaPXq0ipfYT506paZNm6batm2rkpOTVbdu3QJ73cQa+5IlS1T37t3VzTffrLKystT06dPV+fPnlZ++//579dBDD6l27do5537t2rWu+2zevFn17dvXOd5du3ZVRUVFvsQKAI0NeVQwyKPIo7xq7HkUc1QwmKOYo7xq7HMUAIh0CBQXF+ukpCS9cuVK/eOPP+rJkyfr9PR0XV5eLo7funWrTkhI0AsXLtT79+/Xc+bM0U2bNtUlJSWhj338+PG6sLBQ7927Vx84cEA//vjjunnz5vrYsWOhj73aL7/8otu3b6+HDBmiH374YR2EWGO/cOGCzs7O1vn5+XrLli3Oc9i8ebPet29f6GP/+OOPdXJysvOnifurr77Sbdu21dOnT/c17vXr1+vZs2fr1atXazN1rFmz5prjS0tLdWpqqp4xY4bzPl26dKnzvt2wYYNvMQNAY0AeRR4VK/Io8ig/MUcxR8WKOYo5CgDCIBQXzQcMGKCnTZt29fbly5d1u3bt9Pz588XxY8aM0SNHjqy1LScnRz/11FM67LHbLl26pNPS0vRHH32k4yF2E+/AgQP1+++/rydOnBhYIhVr7O+++67u3Lmzrqqq0kGLNXYz9oEHHqi1zSQngwYN0kHxkkg999xzulevXrW2jR07Vufl5d3g6ACgcSGPIo+KFXkUeZSfmKOYo2LFHMUcBQBhEHh5lqqqKrV7927nV9eq3XTTTc7t7du3i/uY7TXHG3l5eXWOD1Pstn///VddvHhRZWRkqHiI/dVXX1WtWrVSkyZNUkGpT+xffPGFys3NdX5lr3Xr1uruu+9Wr7/+urp8+XLoYx84cKCzT/Wv9ZWWljq/apifn6/CLCzvUwBoyMijyKNiRR5FHuUn5ijmqFgxRzFHAUBYJAYdQEVFhfNhZj7cajK3Dx48KO5z/PhxcbzZHvbYbc8//7xTN8z+wAlj7Fu2bFEffPCB2rdvnwpSfWI3yce3336rHn30UScJ+fnnn9XUqVOdJLagoCDUsY8fP97Zb/DgweY3Q9SlS5fU008/rV588UUVZnW9T8+cOaMqKyudmn0AgOtDHkUeFSvyKPIoPzFHMUfFijmKOQoAwiLwb5o3ZgsWLHCarKxZs8ZpEhJmZ8+eVRMmTHCa22RmZqp4c+XKFedbEytWrFD9+vVTY8eOVbNnz1bLly9XYWcarJhvSrzzzjtqz549avXq1WrdunVq7ty5QYcGAEBgyKP8Qx4FxI45yj/MUQCABvlNc/OhnJCQoMrLy2ttN7fbtGkj7mO2xzI+TLFXW7RokZNIffPNN+qee+5Rfos19sOHD6sjR444HbVrJidGYmKiOnTokOrSpUtoj7vpot60aVNnv2o9e/Z0/oXc/BpdUlKSCmvsL730kpPEPvnkk87t3r17q3PnzqkpU6Y4yaD5lb8wqut92qxZM755AAD/EfIo8qhYkUeRR/mJOYo5KlbMUcxRABAWgc/A5gPM/Gvwpk2ban1Am9umLpnEbK853ti4cWOd48MUu7Fw4ULnX443bNigsrOzVRBijb1Hjx6qpKTE+XW96p9Ro0apYcOGOf+flZUV2tiNQYMGOb+mV538GT/99JOTYPmVRNU3dlML0U6WqhPC//dpCaewvE8BoCEjjyKPutGxG+RR/gvL+/R6MUcxR93o2A3mKP+F5X0KADeUDoHi4mKdnJysi4qK9P79+/WUKVN0enq6Pn78uPP3EyZM0LNmzbo6fuvWrToxMVEvWrRIHzhwQBcUFOimTZvqkpKS0Me+YMECnZSUpD///HP9xx9/XP05e/Zs6GO3BdlRPdbYy8rKnM71zzzzjD506JD+8ssvdatWrfS8efNCH7t5fZvYP/nkE11aWqq//vpr3aVLFz1mzBhf4zav0b179zo/ZupYvHix8/9Hjx51/t7EbGKvZmJNTU3VM2fOdN6nhYWFOiEhQW/YsMHXuAGgoSOPIo+KFXkUeZSfmKOYo2LFHMUcBQBhEIqL5sbSpUt1hw4dnCRjwIABeseOHVf/bujQoc6Hdk2fffaZ7tatmzO+V69eet26dToeYu/YsaPzIWT/mA/LsMcepkSqPrFv27ZN5+TkOElM586d9WuvvaYvXboU+tgvXryoX375ZSd5SklJ0VlZWXrq1Kn65MmTvsb83Xffia/d6ljNnyZ2e58+ffo4z9Mc8w8//NDXmAGgsSCPIo+KFXkUeZSfmKOYo2LFHMUcBQBBa2L+c2O/yw4AAAAAAAAAQHwIvKY5AAAAAAAAAABhwUVzAAAAAAAAAAAiuGgOAAAAAAAAAEAEF80BAAAAAAAAAIjgojkAAAAAAAAAABFcNAcAAAAAAAAAIIKL5gAAAAAAAAAARHDRHAAAAAAAAACACC6aAwAAAAAAAAAQwUVzAAAAAAAAAAAiuGgOAAAAAAAAAID6v/8BKcCsfDqw8iEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch Test Results:\n",
      "Correct predictions: 2/3\n",
      "Accuracy: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Batch testing on multiple images\n",
    "def batch_test_model(num_samples=10):\n",
    "    \"\"\"Test model on multiple random samples from test dataset\"\"\"\n",
    "    import os\n",
    "    import random\n",
    "    \n",
    "    print(f\"Testing model on {num_samples} random samples...\")\n",
    "    \n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Create figure for displaying results\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        # Get random test image\n",
    "        test_dir = 'data/test'\n",
    "        emotion_folders = os.listdir(test_dir)\n",
    "        random_emotion = random.choice(emotion_folders)\n",
    "        \n",
    "        if random_emotion in classes:\n",
    "            emotion_path = os.path.join(test_dir, random_emotion)\n",
    "            image_files = [f for f in os.listdir(emotion_path) if f.endswith('.jpg')]\n",
    "            random_image = random.choice(image_files)\n",
    "            image_path = os.path.join(emotion_path, random_image)\n",
    "            \n",
    "            # Load image\n",
    "            image = cv2.imread(image_path)\n",
    "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Preprocess and predict\n",
    "            processed_image = preprocess_image(image)\n",
    "            predicted_emotion, confidence, _ = predict_emotion(processed_image)\n",
    "            \n",
    "            # Check if prediction is correct\n",
    "            is_correct = predicted_emotion == random_emotion\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "            \n",
    "            # Display image\n",
    "            axes[i].imshow(image_rgb)\n",
    "            color = 'green' if is_correct else 'red'\n",
    "            axes[i].set_title(f'True: {random_emotion}\\nPred: {predicted_emotion}\\nConf: {confidence:.2f}', \n",
    "                            color=color, fontsize=8)\n",
    "            axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(num_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "    print(f\"\\nBatch Test Results:\")\n",
    "    print(f\"Correct predictions: {correct_predictions}/{total_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Run batch test\n",
    "print(\"Running batch test on 10 random samples...\")\n",
    "batch_accuracy = batch_test_model(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d8740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative demo without webcam - Interactive image selection\n",
    "def interactive_image_demo():\n",
    "    \"\"\"Interactive demo where you can select images to test\"\"\"\n",
    "    import os\n",
    "    import random\n",
    "    \n",
    "    print(\"Interactive Image Demo - Model1_best\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get all available test images\n",
    "    test_dir = 'data/test'\n",
    "    all_images = []\n",
    "    \n",
    "    for emotion in classes:\n",
    "        emotion_path = os.path.join(test_dir, emotion)\n",
    "        if os.path.exists(emotion_path):\n",
    "            image_files = [f for f in os.listdir(emotion_path) if f.endswith('.jpg')]\n",
    "            for img_file in image_files:\n",
    "                all_images.append((emotion, os.path.join(emotion_path, img_file)))\n",
    "    \n",
    "    print(f\"Found {len(all_images)} test images\")\n",
    "    \n",
    "    # Test on random samples\n",
    "    num_tests = 5\n",
    "    print(f\"\\nTesting on {num_tests} random images...\")\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i in range(num_tests):\n",
    "        # Select random image\n",
    "        true_emotion, image_path = random.choice(all_images)\n",
    "        \n",
    "        # Load and process image\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        processed_image = preprocess_image(image)\n",
    "        \n",
    "        # Predict emotion\n",
    "        predicted_emotion, confidence, all_predictions = predict_emotion(processed_image)\n",
    "        \n",
    "        # Check if correct\n",
    "        is_correct = predicted_emotion == true_emotion\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "        \n",
    "        # Display results\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 4, 1)\n",
    "        plt.imshow(image_rgb)\n",
    "        plt.title(f'Test {i+1}\\nTrue: {true_emotion}', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Processed image\n",
    "        plt.subplot(1, 4, 2)\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        plt.imshow(gray_image, cmap='gray')\n",
    "        plt.title('Processed (48x48)', fontsize=12)\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Prediction bar chart\n",
    "        plt.subplot(1, 4, 3)\n",
    "        emotions = classes\n",
    "        confidences = all_predictions\n",
    "        colors = ['green' if e == predicted_emotion else 'lightblue' for e in emotions]\n",
    "        \n",
    "        bars = plt.bar(emotions, confidences, color=colors)\n",
    "        plt.title(f'Predictions\\nPredicted: {predicted_emotion}\\nConfidence: {confidence:.3f}', fontsize=12)\n",
    "        plt.ylabel('Confidence')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Add confidence values on bars\n",
    "        for bar, conf in zip(bars, confidences):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                    f'{conf:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "        \n",
    "        # Result summary\n",
    "        plt.subplot(1, 4, 4)\n",
    "        result_text = f\"Result: {'✓ CORRECT' if is_correct else '✗ WRONG'}\\n\"\n",
    "        result_text += f\"True: {true_emotion}\\n\"\n",
    "        result_text += f\"Predicted: {predicted_emotion}\\n\"\n",
    "        result_text += f\"Confidence: {confidence:.3f}\\n\"\n",
    "        result_text += f\"Accuracy so far: {correct}/{total} ({correct/total:.1%})\"\n",
    "        \n",
    "        plt.text(0.1, 0.5, result_text, transform=plt.gca().transAxes, \n",
    "                fontsize=12, verticalalignment='center',\n",
    "                bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\" if is_correct else \"lightcoral\"))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Test {i+1}: True={true_emotion}, Predicted={predicted_emotion}, Correct={is_correct}\")\n",
    "    \n",
    "    print(f\"\\nFinal Results:\")\n",
    "    print(f\"Correct: {correct}/{total}\")\n",
    "    print(f\"Accuracy: {correct/total:.1%}\")\n",
    "    \n",
    "    return correct/total\n",
    "\n",
    "# Run the interactive demo\n",
    "interactive_accuracy = interactive_image_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2151c03",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
